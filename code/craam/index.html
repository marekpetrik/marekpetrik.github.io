<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CRAAM: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CRAAM
   &#160;<span id="projectnumber">2.0.0</span>
   </div>
   <div id="projectbrief">Robust and Approximate Markov Decision Processes</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">CRAAM Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction </h2>
<p>Craam is a <b>header-only</b> C++ library for solving Markov decision processes with <em>regular</em>, <em>robust</em>, or <em>optimistic</em> objectives. The optimistic objective is the opposite of robust, in which nature chooses the best possible realization of the uncertain values. The library also provides tools for <em>basic simulation</em>, for constructing MDPs from <em>sample*s, and *value function approximation</em>. Objective functions supported are infinite horizon discounted MDPs, finite horizon MDPs, and stochastic shortest path [Puterman2005]. Some basic stochastic shortest path methods are also supported. The library assumes <em>maximization</em> over actions. The number of states and actions must be finite.</p>
<p>The library is build around two main data structures: MDP and RMDP. <b>MDP</b> is the standard model that consists of states ${S}$ and actions ${A}$. The robust solution for an MDP would satisfy, for example, the following Bellman optimality equation: </p><p class="formulaDsp">
\[ v(s) = \max_{a \in \mathcal{A}} \min_{p \in \Delta} \left\{ \sum_{s&#39;\in\mathcal{S}} p(s&#39;) ( r(s,a,s&#39;) + \gamma \, \, v(s&#39;) ) ~:~ \|p - P(s,a,\cdot) \| \le \psi, \; p \ll P(s,a,\cdot) \right\}~. \]
</p>
<p> Note that $p$ is constrained to be <b>absolutely continuous</b> with respect to $P(s,a,)$. This is a hard requirement for all choices of ambiguity (or uncertainty).</p>
<p>The <b>RMPD</b> model adds a set of <em>outcomes</em> that model possible actions that can be taken by nature. In that case, the robust solution may for example satisfy the following Bellman optimality equation: </p><p class="formulaDsp">
\[ v(s) = \max_{a \in \mathcal{A}} \min_{o \in \mathcal{O}} \sum_{s&#39;\in\mathcal{S}} P(s,a,o,s&#39;) ( r(s,a,o,s&#39;) + \gamma \, v(s&#39;) ) ~. \]
</p>
<p> Using outcomes makes it more convenient to capture correlations between the ambiguity in rewards and the uncertainty in transition probabilities. It also make it much easier to represent uncertainties that lie in small-dimensional vector spaces. The equation above uses the worst outcome, but in general distributions over outcomes are supported.</p>
<p>The available algorithms are <em>value iteration</em> and <em>modified policy iteration</em>. The library support both the plain worst-case outcome method and a worst case with respect to a base distribution.</p>
<h2>Installation and Build Instruction </h2>
<p>See the README.rst</p>
<h2>Getting Started </h2>
<p>See the <a href="http://cs.unh.edu/~mpetrik/code/craam">online documentation</a> or generate it locally as described above.</p>
<p>Unit tests provide some examples of how to use the library. For simple end-to-end examples, see <code>tests/benchmark.cpp</code> and <code>test/dev.cpp</code>. Targets <code>BENCH</code> and <code>DEV</code> build them respectively.</p>
<p>The main models supported are:</p>
<ul>
<li><code><a class="el" href="namespacecraam.html#a9d59929435946aa3b9f860ca5d0b051f" title="Regular MDP with discrete actions and one outcome per action. ">craam::MDP</a></code> : plain MDP with no specific definition of ambiguity (can be used to compute robust solutions anyway)</li>
<li><code><a class="el" href="namespacecraam.html#a05b3d45d32289671966e6ad1057c8a67" title="An uncertain MDP with outcomes and weights. ">craam::RMDP</a></code> : an augmented model that adds nature's actions (so-called outcomes) to the model for convenience</li>
<li><code>craam::impl::MDPIR</code> : an MDP with implementability constraints. See [Petrik2016].</li>
</ul>
<p>The regular value-function based methods are in the header <code><a class="el" href="values_8hpp_source.html">algorithms/values.hpp</a></code> and the robust versions are in in the header <code><a class="el" href="robust__values_8hpp_source.html">algorithms/robust_values.hpp</a></code>. There are 4 main value-function based methods:</p>
<table class="doxtable">
<tr>
<th>Method </th><th>Algorithm  </th></tr>
<tr>
<td><code>solve_vi</code> </td><td>Gauss-Seidel value iteration; runs in a single thread. </td></tr>
<tr>
<td><code>solve_mpi</code> </td><td>Jacobi modified policy iteration; parallelized with OpenMP. Generally, modified policy iteration is vastly more efficient than value iteration. </td></tr>
<tr>
<td><code>rsolve_vi</code> </td><td>Like the value iteration above, but also supports robust, risk-averse, or optimistic objectives. </td></tr>
<tr>
<td><code>rsolve_mpi</code> </td><td>Like the modified policy iteration above, but it also supports robust, risk-averse, optimistic objective. </td></tr>
</table>
<p>These methods can be applied to eithen an MDP or an RMDP.</p>
<p>The header <code><a class="el" href="occupancies_8hpp_source.html">algorithms/occupancies.hpp</a></code> provides tools for converting the MDP to a transition matrix and computing the occupancy frequencies.</p>
<p>There are tools for building simulators and sampling from simulations in the header <code><a class="el" href="Simulation_8hpp_source.html">Simulation.hpp</a></code> and methods for handling samples in <code><a class="el" href="Samples_8hpp_source.html">Samples.hpp</a></code>.</p>
<p>The following is a simple example of formulating and solving a small MDP.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;craam/RMDP.hpp&quot;</span></div><div class="line"><span class="preprocessor">#include &quot;craam/modeltools.hpp&quot;</span></div><div class="line"><span class="preprocessor">#include &quot;craam/algorithms/values.hpp&quot;</span></div><div class="line"></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div><div class="line"></div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacecraam.html">craam</a>;</div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacestd.html">std</a>;</div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main(){</div><div class="line">    <a class="code" href="classcraam_1_1GRMDP.html">MDP</a> mdp(3);</div><div class="line"></div><div class="line">    <span class="comment">// transitions for action 0</span></div><div class="line">    <a class="code" href="namespacecraam.html#ae256dd56144dedc246525e537fe1e241">add_transition</a>(mdp,0,0,0,1,0);</div><div class="line">    <a class="code" href="namespacecraam.html#ae256dd56144dedc246525e537fe1e241">add_transition</a>(mdp,1,0,0,1,1);</div><div class="line">    <a class="code" href="namespacecraam.html#ae256dd56144dedc246525e537fe1e241">add_transition</a>(mdp,2,0,1,1,1);</div><div class="line"></div><div class="line">    <span class="comment">// transitions for action 1</span></div><div class="line">    <a class="code" href="namespacecraam.html#ae256dd56144dedc246525e537fe1e241">add_transition</a>(mdp,0,1,1,1,0);</div><div class="line">    <a class="code" href="namespacecraam.html#ae256dd56144dedc246525e537fe1e241">add_transition</a>(mdp,1,1,2,1,0);</div><div class="line">    <a class="code" href="namespacecraam.html#ae256dd56144dedc246525e537fe1e241">add_transition</a>(mdp,2,1,2,1,1.1);</div><div class="line"></div><div class="line">    <span class="comment">// solve using Jacobi value iteration</span></div><div class="line">    <span class="keyword">auto</span>&amp;&amp; re = <a class="code" href="namespacecraam_1_1algorithms.html#a86b9cda42b7a67c1ee9e7c64cc2a8599">algorithms::solve_mpi</a>(mdp,0.9);</div><div class="line"></div><div class="line">    <span class="keywordflow">for</span>(<span class="keyword">auto</span> v : re.valuefunction){</div><div class="line">        cout &lt;&lt; v &lt;&lt; <span class="stringliteral">&quot; &quot;</span>;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>To compile the file, run:</p>
<div class="fragment"><div class="line">$ g++ -fopenmp -std=c++14 -I&lt;path_to_top_craam_folder&gt; simple.cpp</div></div><!-- fragment --><h2>Supported Use Cases </h2>
<ol type="1">
<li>Formulate an uncertain MDP</li>
<li>Compute a solution to an uncertain MDP</li>
<li>Compute value of a fixed policy</li>
<li>Compute an occupancy frequency</li>
<li>Simulate transitions of an MDP</li>
<li>Construct MDP from samples</li>
<li>Simulate a general domain</li>
</ol>
<h2>General Assumptions </h2>
<ul>
<li>Transition probabilities must be non-negative but do not need to add up to one</li>
<li>Transitions with 0 probabilities may be omitted, except there must be at least one target state in each transition</li>
<li><b>State with no actions</b>: A terminal state with value 0</li>
<li><b>Action with no outcomes</b>: Terminates with an error</li>
<li><b>Outcome with no target states</b>: Terminates with an error</li>
</ul>
<h2>References </h2>
<p>[Filar1997] Filar, J., &amp; Vrieze, K. (1997). Competitive Markov decision processes. Springer.</p>
<p>[Puterman2005] Puterman, M. L. (2005). Markov decision processes: Discrete stochastic dynamic programming. Handbooks in operations research and management …. John Wiley &amp; Sons, Inc.</p>
<p>[Iyengar2005] Iyengar, G. N. G. (2005). Robust dynamic programming. Mathematics of Operations Research, 30(2), 1–29.</p>
<p>[Petrik2014] Petrik, M., Subramanian S. (2014). RAAM : The benefits of robustness in approximating aggregated MDPs in reinforcement learning. In Neural Information Processing Systems (NIPS).</p>
<p>[Petrik2016] Petrik, M., &amp; Luss, R. (2016). Interpretable Policies for Dynamic Product Recommendations. In Uncertainty in Artificial Intelligence (UAI). </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Jul 20 2017 23:37:18 for CRAAM by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
