{
    "docs": [
        {
            "location": "/",
            "text": "Marek Petrik\n\n\n\n\nI am an Assistant Professor in the \nDepartment of Computer Science\n at the \nUniversity of New Hampshire\n. Before, I was a Research Staff Member at the \nIBM's T. J. Watson Research Center\n in Yorktown, NY.  I received my Ph.D. from University of Massachusetts Amherst in 2010. My advisor was: \nShlomo Zilberstein\n. \n\n\nIf you would like to schedule a meeting with me, please see \nmy calendar\n.\n\n\nI'm looking for quantitative PhD students interested in machine learning, reinforcement learning, and mathematical optimization. If this is you, please email me.\n\n\nResearch Interests\n\n\nI am interested in \nrobust data-driven decision making\n. I study methods in reinforcement learning, approximate dynamic programming, robust mathematical optimization, and machine learning. I have worked on applications in domains that include precision agriculture, renewable energy management, supply chains, and others.  \n\n\nI am a member of the \nartificial intelligence research group at UNH\n.\n\n\nSelected Publications\n\n\nSafe Policy Improvement by Minimizing Robust Baseline Regret\n, Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, To appear in \nConference on Neural Information Processing Systems (NIPS)\n, 2016. \n\n\nInterpretable Policies for Dynamic Product Recommendations\n Marek Petrik, Ronny Luss, \nUncertainty in Artificial Intelligence (UAI)\n, 2016.\n\n\nTight Approximations of Dynamic Risk Measures\n, Dan Iancu, Marek Petrik, Dharmashankar Subramanian, \nMathematics of Operations Research\n, 40(3), 2015.\n\n\nFinite-Sample Analysis of Proximal Gradient TD Algorithms\n, Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, Marek Petrik, \nUncertainty in Artificial Intelligence (UAI)\n, 2015, (\nBest Student Paper Award\n). [\nAppendix\n]\n\n\nOptimal Threshold Control for Energy Arbitrage with Degradable Battery Storage\n, Marek Petrik, Xiaojian Wu, \nUncertainty in Artificial Intelligence (UAI)\n, 2015. [\nAppendix\n]\n\n\nRAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning\n, Marek Petrik, Dharmashankar Subramanian, \nConference on Neural Information Processing Systems (NIPS), (spotlight)\n, 2014. [\nFull Paper\n].\n\n\nAll Publications\n\n\nContact Information\n\n\n\n\nEmail: \nmarekpetrik@gmail.com\n\n\nCell Phone: +1-413-230-7479\n\n\nOffice Phone: +1-603-862-2682\n\n\nDepartment of Computer Science\n\n\nKingsbury W233\n\n\nDurham, NH 03824",
            "title": "Home"
        },
        {
            "location": "/#marek-petrik",
            "text": "I am an Assistant Professor in the  Department of Computer Science  at the  University of New Hampshire . Before, I was a Research Staff Member at the  IBM's T. J. Watson Research Center  in Yorktown, NY.  I received my Ph.D. from University of Massachusetts Amherst in 2010. My advisor was:  Shlomo Zilberstein .   If you would like to schedule a meeting with me, please see  my calendar .  I'm looking for quantitative PhD students interested in machine learning, reinforcement learning, and mathematical optimization. If this is you, please email me.",
            "title": "Marek Petrik"
        },
        {
            "location": "/#research-interests",
            "text": "I am interested in  robust data-driven decision making . I study methods in reinforcement learning, approximate dynamic programming, robust mathematical optimization, and machine learning. I have worked on applications in domains that include precision agriculture, renewable energy management, supply chains, and others.    I am a member of the  artificial intelligence research group at UNH .",
            "title": "Research Interests"
        },
        {
            "location": "/#selected-publications",
            "text": "Safe Policy Improvement by Minimizing Robust Baseline Regret , Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, To appear in  Conference on Neural Information Processing Systems (NIPS) , 2016.   Interpretable Policies for Dynamic Product Recommendations  Marek Petrik, Ronny Luss,  Uncertainty in Artificial Intelligence (UAI) , 2016.  Tight Approximations of Dynamic Risk Measures , Dan Iancu, Marek Petrik, Dharmashankar Subramanian,  Mathematics of Operations Research , 40(3), 2015.  Finite-Sample Analysis of Proximal Gradient TD Algorithms , Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, Marek Petrik,  Uncertainty in Artificial Intelligence (UAI) , 2015, ( Best Student Paper Award ). [ Appendix ]  Optimal Threshold Control for Energy Arbitrage with Degradable Battery Storage , Marek Petrik, Xiaojian Wu,  Uncertainty in Artificial Intelligence (UAI) , 2015. [ Appendix ]  RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning , Marek Petrik, Dharmashankar Subramanian,  Conference on Neural Information Processing Systems (NIPS), (spotlight) , 2014. [ Full Paper ].  All Publications",
            "title": "Selected Publications"
        },
        {
            "location": "/#contact-information",
            "text": "Email:  marekpetrik@gmail.com  Cell Phone: +1-413-230-7479  Office Phone: +1-603-862-2682  Department of Computer Science  Kingsbury W233  Durham, NH 03824",
            "title": "Contact Information"
        },
        {
            "location": "/publications/",
            "text": "Publications and Presentations\n\n\n2017\n\n\nRobust Partially-Compressed Least-Squares\n, Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy, AAAI 2017 (to appear)\n\n\n2016\n\n\nSafe Policy Improvement by Minimizing Robust Baseline Regret\n, Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, To appear in \nConference on Neural Information Processing Systems (NIPS)\n, 2016. \n\n\nInterpretable Policies for Dynamic Product Recommendations\n, Marek Petrik, Ronny Luss, \nUncertainty in Artificial Intelligence (UAI)\n, 2016.\n\n\nBuilding an Interpretable Recommender via Loss-Preserving Transformation\n, Amit Dhurandhar, Sechan Oh, Marek Petrik, \n2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)\n.\n\n\nSafe Policy Improvement by Minimizing Robust Baseline Regret\n Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, \n2016 ICML Workshop on Reliable Machine Learning in the Wild\n.\n\n\n2015\n\n\nRobust Policy Optimization with Baseline Guarantees\n, Yinlam Chow, Marek Petrik, Mohammad Ghavamzadeh, \narXiv\n:1506.04514. \n\n\nRobust Partially-Compressed Least-Squares\n, Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy, \narXiv\n:1510.04905. \n\n\nTight Approximations of Dynamic Risk Measures\n, Dan Iancu, Marek Petrik, Dharmashankar Subramanian, \nMathematics of Operations Research\n, 40(3), 2015.\n\n\nFinite-Sample Analysis of Proximal Gradient TD Algorithms\n, Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, Marek Petrik, \nUncertainty in Artificial Intelligence (UAI)\n, 2015, (\nBest Student Paper Award\n). [\nAppendix\n]\n\n\nOptimal Threshold Control for Energy Arbitrage with Degradable Battery Storage\n, Marek Petrik, Xiaojian Wu, \nUncertainty in Artificial Intelligence (UAI)\n, 2015. [\nAppendix\n]\n\n\n2014\n\n\nRAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning\n, Marek Petrik, Dharmashankar Subramanian, \nConference on Neural Information Processing Systems (NIPS), (spotlight)\n, 2014. [\nFull Paper\n].\n\n\nEfficient and Accurate Methods for Updating Generalized Linear Models with Multiple Feature Additions\n, Amit Dhurandhar, Marek Petrik, \nJournal of Machine Learning Research 15:2607-2627\n, 2014.  [\nbib\n]\n\n\nCombining Social Media and Customer Behavior Analytics for Personalized Customer Engagements\n, Markus Ettl, Prateek Jain, Ronny Luss, Marek Petrik, Rajesh Ravi, Chitra Venkatramani, \nIBM Journal of Research Development\n, 58 (5/6) 7:1-7:12, 2014.\n\n\n2013\n\n\nOptimizing Deliveries in Agile Supply Chains with Demand Shocks\n, Francisco Barahona, Markus Ettl, Marek Petrik, Peter Rimshnick, \nWinter Simulation Conference\n, 2013.\n\n\nSolution Methods for Constrained Markov Decision Process with Continuous Probability Modulation\n, Janusz Marecki, Marek Petrik, Dharmashankar Subramanian, \nConference on Uncertainty in Artificial Intelligence (UAI)\n, 2013.\n\n\n2012\n\n\nAn Approximate Solution Method for Large Risk-Averse Markov Decision Processes\n, Marek Petrik and Dharmashankar Subramanian. \nConference on Uncertainty in Artificial Intelligence (UAI)\n, 2012.\n\n\nDistributionally Robust Approach to Approximate Dynamic Programming\n, Marek Petrik, \nInternational Conference on Machine Learning (ICML)\n, 2012. Also presented at \nEuropean Workshop on Reinforcement Learning\n, 2012. \nExtended Technical Report (includes proofs)\n.\n\n\nOptimizing the end-to-end value chain through demand shaping and advanced customer analytics\n, Brenda Dietrich, Markus Ettl, Roger D. Lederman, Marek Petrik, \n11th International Symposium on Process Systems Engineering\n, 2012.\n\n\n2011\n\n\nThe Price of Dynamic Inconsistency for Distortion Risk Measures\n, Pu Huang, Dan Iancu, Marek Petrik, Dharmashankar Subramanian. \nTechnical Report\n, 2011.\n\n\nLinear Dynamic Programs for Resource Management\n, Marek Petrik and Shlomo Zilberstein, \nConference on Artificial Intelligence (AAAI) [Computational Sustainability Track]\n, 2011.\n\n\nRobust Approximate Bilinear Programming for Value Function Approximation\n, Marek Petrik and Shlomo Zilberstein, \nJournal of Machine Learning Research 12(Oct):3027-3063\n, 2011.\n\n\n2010\n\n\nOptimization-based Approximate Dynamic Programming\n, Marek Petrik, \nPh.D.  Dissertation\n, 2010. Also, the \noriginal double-spaced version\n, and the \ndefense presentation\n.\n\n\nFeature Selection Using Regularization in Approximate Linear Program for Markov Decision Processes\n, Marek Petrik, Gavin Taylor, Ron Parr, Shlomo Zilberstein. \nInternational Conference on Machine Learning (ICML) 27\n, 2010. \nTechnical Report (includes proofs and algorithms): arXiv 1005.1860\n.\n\n\n2009\n\n\nRobust Value Function Approximation Using Bilinear Programming\n, Marek Petrik and Shlomo Zilberstein, \nConference on Neural Information Processing Systems (NIPS) 22 (spotlight)\n, 2009. \nTechnical Report (includes proofs) UM-CS-2009-052\n.\n\n\nA Bilinear Programming Approach for Multiagent Planning\n, Marek Petrik and Shlomo Zilberstein, \nJournal of Artificial Intelligence Research 35:235-274\n, 2009.\n\n\nHybrid Least-Squares Algorithms for Approximate Policy Evaluation\n, Jeff Johns, Marek Petrik, Sridhar Mahadevan, \nEuropean Conference on Machine Learning\n, and \nMachine Learning\n journal, 2009.\n\n\nConstraint Relaxation in Approximate Linear Programs\n, Marek Petrik and Shlomo Zilberstein, \nInternational Conference on Machine Learning (ICML)\n, 2009.\n\n\nRobust Approximate Optimization for Large Scale Planning Problems\n, Marek Petrik, \nAAAI Doctoral Consortium\n, 2009.\n\n\nBlood Management Using Approximate Linear Programming\n, Marek Petrik and Shlomo Zilberstein, \nPresented at INFORMS Computing Society Meeting, Charleston, SC\n, 2009.\n\n\n2008\n\n\nA Successive Approximation Algorithm for Coordination Problems\n, Marek Petrik and Shlomo Zilberstein, \n9th International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale, Florida\n, 2008.\n\n\nBiasing Approximate Dynamic Programming with a Lower Discount Factor\n,Marek Petrik and Bruno Scherrer, \nConference on Neural Information Processing Systems (NIPS)\n, 2008.\n\n\nLearning Heuristic Functions Through Approximate Linear Programming\n, Marek Petrik and Shlomo Zilberstein, \nInternational Conference on Automated Planning and Scheduling (ICAPS)\n, 2008.\n\n\nInteraction Structure and Dimensionality Reduction in Decentralized MDPs\n Martin Allen, Marek Petrik, Shlomo Zilberstein, \nThe National Conference on Artificial Intelligence (AAAI)\n, 2008. \nExtented technical report #UM-CS-2008-11\n.\n\n\n2007\n\n\nAnytime Coordination Using Separable Bilinear Programs\n, Marek Petrik, Shlomo Zilberstein, \nNational Conference on Artificial Intelligence (AAAI)\n, 2007.\n\n\nAn Analysis of Laplacian Methods for Value Function Approximation in MDPs\n, Marek Petrik, \nInternational Joint Conference on Artificial Intelligence (IJCAI)\n, 2007.\n\n\nAverage-Reward Decentralized Markov Decision Processes\n, Marek Petrik, Shlomo Zilberstein, \nInternational Joint Conference on Artificial Intelligence (IJCAI)\n, 2007.\n\n\n2006\n\n\nLearning Parallel Portfolios of Algorithms\n, Marek Petrik, Shlomo Zilberstein, \nAnnals of Mathematics and Artificial Intelligence, 48(1-2):85-106\n, 2006\n\n\nLearning Static Parallel Portfolios of Algorithms\n, Marek Petrik, Shlomo Zilberstein, \nInternational Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale\n, 2006.\n\n\nLearning Parallel Portfolios of Algorithms\n, Marek Petrik, Diploma Thesis at Univerzita Komenskeho, June 7th 2005.  The \ncode\n, and \npresentation\n are also available.\n\n\nStatistically Optimal Combination of Algorithms\n, Marek Petrik, \nSOFSEM\n, 2005. (Best Student Poster).",
            "title": "Publications"
        },
        {
            "location": "/publications/#publications-and-presentations",
            "text": "",
            "title": "Publications and Presentations"
        },
        {
            "location": "/publications/#2017",
            "text": "Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy, AAAI 2017 (to appear)",
            "title": "2017"
        },
        {
            "location": "/publications/#2016",
            "text": "Safe Policy Improvement by Minimizing Robust Baseline Regret , Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, To appear in  Conference on Neural Information Processing Systems (NIPS) , 2016.   Interpretable Policies for Dynamic Product Recommendations , Marek Petrik, Ronny Luss,  Uncertainty in Artificial Intelligence (UAI) , 2016.  Building an Interpretable Recommender via Loss-Preserving Transformation , Amit Dhurandhar, Sechan Oh, Marek Petrik,  2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016) .  Safe Policy Improvement by Minimizing Robust Baseline Regret  Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh,  2016 ICML Workshop on Reliable Machine Learning in the Wild .",
            "title": "2016"
        },
        {
            "location": "/publications/#2015",
            "text": "Robust Policy Optimization with Baseline Guarantees , Yinlam Chow, Marek Petrik, Mohammad Ghavamzadeh,  arXiv :1506.04514.   Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy,  arXiv :1510.04905.   Tight Approximations of Dynamic Risk Measures , Dan Iancu, Marek Petrik, Dharmashankar Subramanian,  Mathematics of Operations Research , 40(3), 2015.  Finite-Sample Analysis of Proximal Gradient TD Algorithms , Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, Marek Petrik,  Uncertainty in Artificial Intelligence (UAI) , 2015, ( Best Student Paper Award ). [ Appendix ]  Optimal Threshold Control for Energy Arbitrage with Degradable Battery Storage , Marek Petrik, Xiaojian Wu,  Uncertainty in Artificial Intelligence (UAI) , 2015. [ Appendix ]",
            "title": "2015"
        },
        {
            "location": "/publications/#2014",
            "text": "RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning , Marek Petrik, Dharmashankar Subramanian,  Conference on Neural Information Processing Systems (NIPS), (spotlight) , 2014. [ Full Paper ].  Efficient and Accurate Methods for Updating Generalized Linear Models with Multiple Feature Additions , Amit Dhurandhar, Marek Petrik,  Journal of Machine Learning Research 15:2607-2627 , 2014.  [ bib ]  Combining Social Media and Customer Behavior Analytics for Personalized Customer Engagements , Markus Ettl, Prateek Jain, Ronny Luss, Marek Petrik, Rajesh Ravi, Chitra Venkatramani,  IBM Journal of Research Development , 58 (5/6) 7:1-7:12, 2014.",
            "title": "2014"
        },
        {
            "location": "/publications/#2013",
            "text": "Optimizing Deliveries in Agile Supply Chains with Demand Shocks , Francisco Barahona, Markus Ettl, Marek Petrik, Peter Rimshnick,  Winter Simulation Conference , 2013.  Solution Methods for Constrained Markov Decision Process with Continuous Probability Modulation , Janusz Marecki, Marek Petrik, Dharmashankar Subramanian,  Conference on Uncertainty in Artificial Intelligence (UAI) , 2013.",
            "title": "2013"
        },
        {
            "location": "/publications/#2012",
            "text": "An Approximate Solution Method for Large Risk-Averse Markov Decision Processes , Marek Petrik and Dharmashankar Subramanian.  Conference on Uncertainty in Artificial Intelligence (UAI) , 2012.  Distributionally Robust Approach to Approximate Dynamic Programming , Marek Petrik,  International Conference on Machine Learning (ICML) , 2012. Also presented at  European Workshop on Reinforcement Learning , 2012.  Extended Technical Report (includes proofs) .  Optimizing the end-to-end value chain through demand shaping and advanced customer analytics , Brenda Dietrich, Markus Ettl, Roger D. Lederman, Marek Petrik,  11th International Symposium on Process Systems Engineering , 2012.",
            "title": "2012"
        },
        {
            "location": "/publications/#2011",
            "text": "The Price of Dynamic Inconsistency for Distortion Risk Measures , Pu Huang, Dan Iancu, Marek Petrik, Dharmashankar Subramanian.  Technical Report , 2011.  Linear Dynamic Programs for Resource Management , Marek Petrik and Shlomo Zilberstein,  Conference on Artificial Intelligence (AAAI) [Computational Sustainability Track] , 2011.  Robust Approximate Bilinear Programming for Value Function Approximation , Marek Petrik and Shlomo Zilberstein,  Journal of Machine Learning Research 12(Oct):3027-3063 , 2011.",
            "title": "2011"
        },
        {
            "location": "/publications/#2010",
            "text": "Optimization-based Approximate Dynamic Programming , Marek Petrik,  Ph.D.  Dissertation , 2010. Also, the  original double-spaced version , and the  defense presentation .  Feature Selection Using Regularization in Approximate Linear Program for Markov Decision Processes , Marek Petrik, Gavin Taylor, Ron Parr, Shlomo Zilberstein.  International Conference on Machine Learning (ICML) 27 , 2010.  Technical Report (includes proofs and algorithms): arXiv 1005.1860 .",
            "title": "2010"
        },
        {
            "location": "/publications/#2009",
            "text": "Robust Value Function Approximation Using Bilinear Programming , Marek Petrik and Shlomo Zilberstein,  Conference on Neural Information Processing Systems (NIPS) 22 (spotlight) , 2009.  Technical Report (includes proofs) UM-CS-2009-052 .  A Bilinear Programming Approach for Multiagent Planning , Marek Petrik and Shlomo Zilberstein,  Journal of Artificial Intelligence Research 35:235-274 , 2009.  Hybrid Least-Squares Algorithms for Approximate Policy Evaluation , Jeff Johns, Marek Petrik, Sridhar Mahadevan,  European Conference on Machine Learning , and  Machine Learning  journal, 2009.  Constraint Relaxation in Approximate Linear Programs , Marek Petrik and Shlomo Zilberstein,  International Conference on Machine Learning (ICML) , 2009.  Robust Approximate Optimization for Large Scale Planning Problems , Marek Petrik,  AAAI Doctoral Consortium , 2009.  Blood Management Using Approximate Linear Programming , Marek Petrik and Shlomo Zilberstein,  Presented at INFORMS Computing Society Meeting, Charleston, SC , 2009.",
            "title": "2009"
        },
        {
            "location": "/publications/#2008",
            "text": "A Successive Approximation Algorithm for Coordination Problems , Marek Petrik and Shlomo Zilberstein,  9th International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale, Florida , 2008.  Biasing Approximate Dynamic Programming with a Lower Discount Factor ,Marek Petrik and Bruno Scherrer,  Conference on Neural Information Processing Systems (NIPS) , 2008.  Learning Heuristic Functions Through Approximate Linear Programming , Marek Petrik and Shlomo Zilberstein,  International Conference on Automated Planning and Scheduling (ICAPS) , 2008.  Interaction Structure and Dimensionality Reduction in Decentralized MDPs  Martin Allen, Marek Petrik, Shlomo Zilberstein,  The National Conference on Artificial Intelligence (AAAI) , 2008.  Extented technical report #UM-CS-2008-11 .",
            "title": "2008"
        },
        {
            "location": "/publications/#2007",
            "text": "Anytime Coordination Using Separable Bilinear Programs , Marek Petrik, Shlomo Zilberstein,  National Conference on Artificial Intelligence (AAAI) , 2007.  An Analysis of Laplacian Methods for Value Function Approximation in MDPs , Marek Petrik,  International Joint Conference on Artificial Intelligence (IJCAI) , 2007.  Average-Reward Decentralized Markov Decision Processes , Marek Petrik, Shlomo Zilberstein,  International Joint Conference on Artificial Intelligence (IJCAI) , 2007.",
            "title": "2007"
        },
        {
            "location": "/publications/#2006",
            "text": "Learning Parallel Portfolios of Algorithms , Marek Petrik, Shlomo Zilberstein,  Annals of Mathematics and Artificial Intelligence, 48(1-2):85-106 , 2006  Learning Static Parallel Portfolios of Algorithms , Marek Petrik, Shlomo Zilberstein,  International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale , 2006.  Learning Parallel Portfolios of Algorithms , Marek Petrik, Diploma Thesis at Univerzita Komenskeho, June 7th 2005.  The  code , and  presentation  are also available.  Statistically Optimal Combination of Algorithms , Marek Petrik,  SOFSEM , 2005. (Best Student Poster).",
            "title": "2006"
        },
        {
            "location": "/code/",
            "text": "Code\n\n\nSource code for some results in my publications.\n\n\nRobust Markov Decision Processes\n\n\nA C++ library for solving robust Markov decision processes. It also provides functionality for implementing simple MDP-based stochastic simulators. Can be used as a basis for reinforcement learning algorithms.\n\n\nRobust Aggregation for MDPs\n\n\nA Python library for simulating and solving large MDPs via an approximation by robust MDPs. Implements several old and new benchmark domains.",
            "title": "Code"
        },
        {
            "location": "/code/#code",
            "text": "Source code for some results in my publications.",
            "title": "Code"
        },
        {
            "location": "/code/#robust-markov-decision-processes",
            "text": "A C++ library for solving robust Markov decision processes. It also provides functionality for implementing simple MDP-based stochastic simulators. Can be used as a basis for reinforcement learning algorithms.",
            "title": "Robust Markov Decision Processes"
        },
        {
            "location": "/code/#robust-aggregation-for-mdps",
            "text": "A Python library for simulating and solving large MDPs via an approximation by robust MDPs. Implements several old and new benchmark domains.",
            "title": "Robust Aggregation for MDPs"
        },
        {
            "location": "/teaching/adv_topics_ml_16/",
            "text": "CS980: Advanced Machine Learning\n\n\nWhere\n\n\nKingsbury N233\n\n\nWhen\n\n\nMW  3:40pm - 5:00pm\n\n\nWhat\n\n\nIn this seminar, we will cover how to make good decisions using machine learning. \nReinforcement learning\n and \nmulti-armed bandits\n are just some of the methods that combine decision making with machine learning. These methods play a crucial role in countless real problems such as when personalizing websites, navigating robots, managing supply chains and revenue, and even when playing Go! \n\n\nWe will cover how to trade off \nexploitation\n and \nexploration\n; that is when to act with the current data versus acquiring additional data to make a better decision later. The exploration/exploitation tradeoff is a key challenge in reinforcement learning. We will also cover the related issues of \nreliability\n and \nrobustness\n in machine learning.\n\n\nSome of the specific topics we will cover are:\n\n\n\n\nA/B testing and randomized experiments\n\n\nOptimizer's curse\n\n\nThompson sampling, UCB, EXP3\n\n\n(Robust) Markov decision processes\n\n\nGittins and Whittle indices\n\n\nTree and policy search\n\n\nCovariate shift\n\n\n\n\nThe class will start with an overview of fundamental problems, methods, and techniques and will continue with student presentations and a research project.\n\n\nPre-requisites\n\n\nStatistics and some linear algebra. Machine learning is not a pre-requisite. We will cover topics that are closely related to ML but do not depend on it.\n\n\nMore details\n\n\nPlease see the class website: \nhttps://bitbucket.org/2016advml/class/wiki",
            "title": "2016: Advanced ML"
        },
        {
            "location": "/teaching/adv_topics_ml_16/#cs980-advanced-machine-learning",
            "text": "",
            "title": "CS980: Advanced Machine Learning"
        },
        {
            "location": "/teaching/adv_topics_ml_16/#where",
            "text": "Kingsbury N233",
            "title": "Where"
        },
        {
            "location": "/teaching/adv_topics_ml_16/#when",
            "text": "MW  3:40pm - 5:00pm",
            "title": "When"
        },
        {
            "location": "/teaching/adv_topics_ml_16/#what",
            "text": "In this seminar, we will cover how to make good decisions using machine learning.  Reinforcement learning  and  multi-armed bandits  are just some of the methods that combine decision making with machine learning. These methods play a crucial role in countless real problems such as when personalizing websites, navigating robots, managing supply chains and revenue, and even when playing Go!   We will cover how to trade off  exploitation  and  exploration ; that is when to act with the current data versus acquiring additional data to make a better decision later. The exploration/exploitation tradeoff is a key challenge in reinforcement learning. We will also cover the related issues of  reliability  and  robustness  in machine learning.  Some of the specific topics we will cover are:   A/B testing and randomized experiments  Optimizer's curse  Thompson sampling, UCB, EXP3  (Robust) Markov decision processes  Gittins and Whittle indices  Tree and policy search  Covariate shift   The class will start with an overview of fundamental problems, methods, and techniques and will continue with student presentations and a research project.",
            "title": "What"
        },
        {
            "location": "/teaching/adv_topics_ml_16/#pre-requisites",
            "text": "Statistics and some linear algebra. Machine learning is not a pre-requisite. We will cover topics that are closely related to ML but do not depend on it.",
            "title": "Pre-requisites"
        },
        {
            "location": "/teaching/adv_topics_ml_16/#more-details",
            "text": "Please see the class website:  https://bitbucket.org/2016advml/class/wiki",
            "title": "More details"
        },
        {
            "location": "/teaching/intro_ml_17/",
            "text": "CS780 / CS880: Introduction to Machine Learning\n\n\nWhen and Where\n\n\nTue & Thu, 12:40 pm - 2:00 pm\nKingsbury N133\n\n\nSee \nclass overview\n for more information on textbooks, syllabus, assignments, office hours, and grading.\n\n\nAssignments\n\n\nPlease use \nPiazza\n for questions about assignments.\n\n\n\n\n\n\n\n\nAssignment\n\n\nDue Date\n\n\n\n\n\n\n\n\n\n\nAssignment 1\n\n\n2/14/17 at 12:40PM\n\n\n\n\n\n\nAssignment 2\n\n\n2/21/17 at 12:40PM\n\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\n\n\n\nDate\n\n\nSlides\n\n\nReading\n\n\nNotebooks\n\n\n\n\n\n\n\n\n\n\n1/26\n\n\nStatistical learning\n\n\nISL 1,2\n\n\n(html)\n \n(RMD)\n\n\n\n\n\n\n1/31\n\n\nLinear regression I\n\n\nISL 3.1-2\n\n\n(html)\n \n(RMD)\n\n\n\n\n\n\n2/02\n\n\nNo class\n\n\n\n\n\n\n\n\n\n\n2/07\n\n\nLinear regression II\n\n\nISL 3.3-6\n\n\n\n\n\n\n\n\n2/09\n\n\nLogistic regression\n\n\nISL 4.1-3\n\n\n(html)\n(RMD)\n\n\n\n\n\n\n2/14\n\n\nLDA, QDA, Bayes\n\n\nISL 4.4-6\n\n\n\n\n\n\n\n\n\n\nProject\n\n\nSee the \nproject overview\n for details on the details of deliverables. The deliverable are due by the end of the day (midnight).\n\n\n| Date      | Deliverable                                           | Page Limit    |\n| -------   | ------------                                          | ---------+    |\n| 2/23      |  Project description and data sources                 | 1             |\n| 3/02      |  Evaluation methodology                               | 1             |\n| 3/23      |  Method and literature overview                       | 2             |\n| 4/06      |  Preliminary results                                  | 3             |\n| 4/27      |  Final report                                         | 7             |\n\n\nClass Content\n\n\nThe goal of this class is to teach you how to use \nmachine learning\n to \nunderstand data\n and \nmake predictions\n in practice. The class will cover the fundamental concepts and algorithms in machine learning and data science as well as a wide variety of practical algorithms. The main topics we will cover are:\n\n\n\n\nThe maximum likelihood principle\n\n\nRegression\n: Linear regression\n\n\nClassification\n: Logistic regression and linear discriminant analysis\n\n\nCross-validation, bootstrap, and over-fitting\n\n\nModel selection\n: Regularization, Lasso\n\n\nNonlinear models\n: Decision trees, Support vector machines\n\n\nUnsupervised\n: Principal component analysis, k-means\n\n\nAdvanced topics\n: Bayes nets and deep learning\n\n\n\n\nThe graduate version of the class will cover the same topics in greater depth.\n\n\nTextbooks\n\n\n\n\n\n\nMain reference:\nISL: \nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning\n\n\n\n\n\n\nMore in-depth material:\nESL: \nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer Series in Statistics (2nd ed.)\n\n\n\n\n\n\nSee \nclass overview\n for more information on textbook.\n\n\nProgramming Language\n\n\nThe class will involve hand-on data analysis using machine learning methods. The recommended language for programming assignments is \nR\n which is excellent tool for statistical analysis and machine learning. \nNo prior knowledge of R is needed or expected\n; the book and lecture will cover a gentle introduction to the language. Experienced students may also choose other alternatives, such as Python or Matlab.\n\n\nPre-requisites\n\n\nBasic programming skills (scripting languages like Python are OK) and some familiarity with statistics and calculus. If in doubt, please email me.",
            "title": "2017: Intro to ML"
        },
        {
            "location": "/teaching/intro_ml_17/#cs780-cs880-introduction-to-machine-learning",
            "text": "",
            "title": "CS780 / CS880: Introduction to Machine Learning"
        },
        {
            "location": "/teaching/intro_ml_17/#when-and-where",
            "text": "Tue & Thu, 12:40 pm - 2:00 pm\nKingsbury N133  See  class overview  for more information on textbooks, syllabus, assignments, office hours, and grading.",
            "title": "When and Where"
        },
        {
            "location": "/teaching/intro_ml_17/#assignments",
            "text": "Please use  Piazza  for questions about assignments.     Assignment  Due Date      Assignment 1  2/14/17 at 12:40PM    Assignment 2  2/21/17 at 12:40PM",
            "title": "Assignments"
        },
        {
            "location": "/teaching/intro_ml_17/#syllabus",
            "text": "Date  Slides  Reading  Notebooks      1/26  Statistical learning  ISL 1,2  (html)   (RMD)    1/31  Linear regression I  ISL 3.1-2  (html)   (RMD)    2/02  No class      2/07  Linear regression II  ISL 3.3-6     2/09  Logistic regression  ISL 4.1-3  (html) (RMD)    2/14  LDA, QDA, Bayes  ISL 4.4-6",
            "title": "Syllabus"
        },
        {
            "location": "/teaching/intro_ml_17/#project",
            "text": "See the  project overview  for details on the details of deliverables. The deliverable are due by the end of the day (midnight).  | Date      | Deliverable                                           | Page Limit    |\n| -------   | ------------                                          | ---------+    |\n| 2/23      |  Project description and data sources                 | 1             |\n| 3/02      |  Evaluation methodology                               | 1             |\n| 3/23      |  Method and literature overview                       | 2             |\n| 4/06      |  Preliminary results                                  | 3             |\n| 4/27      |  Final report                                         | 7             |",
            "title": "Project"
        },
        {
            "location": "/teaching/intro_ml_17/#class-content",
            "text": "The goal of this class is to teach you how to use  machine learning  to  understand data  and  make predictions  in practice. The class will cover the fundamental concepts and algorithms in machine learning and data science as well as a wide variety of practical algorithms. The main topics we will cover are:   The maximum likelihood principle  Regression : Linear regression  Classification : Logistic regression and linear discriminant analysis  Cross-validation, bootstrap, and over-fitting  Model selection : Regularization, Lasso  Nonlinear models : Decision trees, Support vector machines  Unsupervised : Principal component analysis, k-means  Advanced topics : Bayes nets and deep learning   The graduate version of the class will cover the same topics in greater depth.",
            "title": "Class Content"
        },
        {
            "location": "/teaching/intro_ml_17/#textbooks",
            "text": "Main reference:\nISL:  James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning    More in-depth material:\nESL:  Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer Series in Statistics (2nd ed.)    See  class overview  for more information on textbook.",
            "title": "Textbooks"
        },
        {
            "location": "/teaching/intro_ml_17/#programming-language",
            "text": "The class will involve hand-on data analysis using machine learning methods. The recommended language for programming assignments is  R  which is excellent tool for statistical analysis and machine learning.  No prior knowledge of R is needed or expected ; the book and lecture will cover a gentle introduction to the language. Experienced students may also choose other alternatives, such as Python or Matlab.",
            "title": "Programming Language"
        },
        {
            "location": "/teaching/intro_ml_17/#pre-requisites",
            "text": "Basic programming skills (scripting languages like Python are OK) and some familiarity with statistics and calculus. If in doubt, please email me.",
            "title": "Pre-requisites"
        },
        {
            "location": "/teaching/",
            "text": "Teaching\n\n\nSpring 2017\n\n\nIntroduction to Machine Learning\n\n\nFall 2016\n\n\nAdvanced Topics in Machine Learning",
            "title": "List All"
        },
        {
            "location": "/teaching/#teaching",
            "text": "",
            "title": "Teaching"
        },
        {
            "location": "/teaching/#spring-2017",
            "text": "Introduction to Machine Learning",
            "title": "Spring 2017"
        },
        {
            "location": "/teaching/#fall-2016",
            "text": "Advanced Topics in Machine Learning",
            "title": "Fall 2016"
        },
        {
            "location": "/CV/",
            "text": "CV\n\n\nPlease see the \nPDF\n version of my CV.",
            "title": "CV"
        },
        {
            "location": "/CV/#cv",
            "text": "Please see the  PDF  version of my CV.",
            "title": "CV"
        },
        {
            "location": "/tutorials/risk/",
            "text": "AAAI-2017: Tutorial on Risk-averse Decision Making and Control\n\n\nPresenters\n:\n\n\n\n\nMarek Petrik, University of New Hampshire\n\n\nMohammad Ghavamzadeh, Adobe Research\n\n\n\n\nLocation\n: Continental 1-3, Ballroom Level\n\n\nSchedule\n\n\n\n\n9:00AM - 9:20AM\n: Introduction to risk-averse modeling\n\n\n9:20AM - 9:40AM\n: Value at Risk and Conditional Value at Risk\n\n\n9:40AM - 9:50AM\n: \nBreak\n\n\n9:50AM - 10:30AM\n: Coherent Measures of Risk: Properties and methods\n\n\n10:30AM - 11:00AM\n: \nCoffee break\n\n\n11:00AM - 12:00PM\n: Risk-averse reinforcement learning\n\n\n12:00PM - 12:15PM\n: \nBreak\n\n\n12:15PM - 12:45PM\n: Time consistent measures of risk\n\n\n\n\nSlides\n\n\n\n\nCoherent Measures of Risk (Dynamic and Static)\n\n\nRisk-averse Reinforcement Learning\n\n\n\n\nDetailed Description\n\n\nTraditional decision methods in artificial intelligence focus on maximizing the expected return (or minimizing the expected cost). This is appropriate when the decision-makers are risk-neutral. Yet, many decision-makers are \nrisk-sensitive\n and are willing to give up some of the expected reward in order to protect against large losses. The desire to avoid risk when making decision was recognized early on, but developing appropriate models to capture risk has been challenging. Useful models of risk-aversion must be easy to understand and interpret for decision-makers; but they also must be general, flexible, and more importantly, they must produce tractable optimization problems.\n\n\nThe classical approach to modeling risk aversion is to use expected utilities, but they are difficult to specify and significantly complicate optimization methods. This tutorial focuses on the new approach to risk-aversion which is based convex measures of risk. Convex measures of risk replace the expectation operator by a more general operator which puts more weight on negative outcomes. Perhaps the most well-known risk measure is CVaR, which at level alpha computes the expectation of the lowest alpha-quantile of returns. \n\n\nThere has been tremendous progress in developing the theory and practice of risk measures since their introduction in the late 1990s. Researchers and practitioners have proposed and used many other risk measures besides CVaR and many stochastic optimization methods now work with convex risk measures. Due to the ease of modeling and optimization with them, convex risk measures have become the standard method for capturing risk sensitivity in operations research. Robust optimization, a related concept for modeling risk-aversion and avoidance, has flourished similarly. \n\n\nIn recent years, there has been growing interest in developing risk averse decision-making methods in artificial intelligence and machine learning. Risk-aversion is required to make machine learning relevant in many practical settings since solutions from risk neutral methods are often too risky in mission-critical problems. Convex risk measures and robust optimization are now being used in methods that range from classification, through multi-armed bandits, to reinforcement learning. While the general concept of risk measures is relatively simple, their true power can only be realized through deeper understanding. For example, integrating risk aversion with sequential decision-making requires overcoming a full set of challenges concerning time consistency. Our tutorial will shed light on these issues and provide numerous pointers for further research. \n\n\nGoals and Target Audience\n\n\nThis tutorial will introduce the tools and methodology of convex risk measures and robust optimization, developed in operations research and stochastic finance, to the machine learning community. The goal is to make these often complex results accessible and provide a starting point for people interested in exploring this research direction in greater detail. We will introduce basic concepts of risk measures and robust optimization, describe connections and advantages w.r.t. the existing methods, and describe how risk aversion can be used in sequential decision problems. \n\n\nThis tutorial should be of interest to researchers in any area that involves decision-making or control. This in particular includes the reinforcement learning and online learning communities, in which the application of risk aversion presents the most pitfalls. Risk aversion can also be important in classification and regression problems as several recent publications attest to. We plan to introduce the general risk-modeling framework and assume just knowledge of measure theoretical concepts, linear algebra, and basic optimization. \n\n\nReferences\n\n\nCoherent risk measures\n\n\n\n\nPhilippe Artzner, Freddy Delbaen, Jean-marc Eber, and David Heath. Coherent Measures of Risk. Mathematical Finance, 9(June 1996):203\u2013228, 1999\n\n\nR. Tyrrell Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk, 2:21\u201341, 2000\n\n\nR. Tyrrell Rockafellar and Stanislav Uryasev. Conditional Value-At-Risk for General Loss Distributions. Journal of Banking and Finance, 26(7):1443\u20131471, \n\n\nAlexander Schied. Risk measures and robust optimization problems. In Symposium on Probability and Stochastic Processes, 2004\n\n\nAharon Ben-Tal and Marc Teboulle. An Old-New Concept of Convex Risk Measures: The Optimized Certainty Equivalent. Mathematical Finance, 17:449\u2013476, 2007\n\n\nAlexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Programming. SIAM, 2009\n\n\nAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization. Princeton University Press, 2009\n\n\nHans Follmer and Alexander Schied. Stochastic Finance: An Introduction in Discrete Time. Walter de Gruyter, 3rd edition, 2011\n\n\nA. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013\n\n\n\n\nSequential decision making\n\n\n\n\nV. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems & Control Letters, 44:339\u2013346, 2001\n\n\nV. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27:294\u2013311, 2002\n\n\nPeter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints. Journal of Artificial Intelligence Research, 24:81\u2013108, 2005\n\n\nE. Delage and S. Mannor. Percentile Optimization for Markov Decision Processes with Parameter Uncertainty. Operations Research, 58(1):203\u2013213, aug 2009\n\n\nAndrzej Ruszczynski. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming B, 125(2):235\u2013261, jul 2010\n\n\nJanusz Marecki and Pradeep Varakantham. Risk-Sensitive Planning in Partially Observable Environments. In Conference on Autonomous Agents and Multiagent Systems, 2010\n\n\nAlexander Shapiro. Analysis of Stochastic Dual Dynamic Programming Method. European Journal of Operational Research, 209(1):63\u201372, 2011\n\n\nMarek Petrik and Dharmashankar Subramanian. An approximate solution method for large risk-averse Markov decision processes. In Uncertainty in Artificial Intelligence(UAI), 2012\n\n\nWolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust Markov decision processes. Mathematics of Operations Research, 38(1):153\u2013183, apr 2013\n\n\nA. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013\n\n\nVincent Guigues. SDDP for some interstage dependent risk-averse problems and application to hydro-thermal planning. Computational Optimization and Applications, pages 1\u201326, jul 2013\n\n\nAviv Tamar, Dotan Di Castro, and Shie Mannor. Temporal Difference Methods for the Variance of the Reward To Go. Proceedings of the 30th International Conference on Machine Learning, 28:495\u2013503, 2013\n\n\nL.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In Advances in Neural Information Processing Systems, pages 252\u2013260, 2013\n\n\nYinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR Optimization in MDPs. In Neural Information Processing Systems (NIPS), pages 3509\u20133517, 2014\n\n\nDan A Iancu, Marek Petrik, and Dharmashankar Subramanian. Tight approximations of dynamic risk measures. Mathematics of Operations Research, 2015\n\n\nJavier Garc and Fernando Fern. A Comprehensive Survey on Safe Reinforcement Learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015\n\n\nAviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the CVaR via Sampling. In AAAI Conference on Artificial Intelligence, pages 2993\u20132999, 2015\n\n\nAviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy Gradient for Coherent Risk Measures. In Neural Information Processing Systems (NIPS), pages 1468\u20131476, 2015\n\n\nYinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-Sensitive and Robust Decision-Making : a CVaR Optimization Approach. In Neural Information Processing Systems (NIPS), 2015\n\n\nAviv Tamar and Dotan Di Castro. Learning the Variance of the Reward-To-Go. Journal of Machine Learning Research, 17:1\u201336, 2016\n\n\nL.A. Prashanth and Mohammad Ghavamzadeh. Variance-constrained Actor-Critic Algorithms for Discounted and Average Reward MDPs. Machine Learning Journal, 2016\n\n\nY. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, to appear, 2016\n\n\n\n\nOther machine learning\n\n\n\n\nStefano Ermon, Jon Conrad, Carla Gomes, and Bart Selman. Risk-Sensitive Policies for Sustainable Renewable Resource Allocation. Twenty-Second International Joint Conference on Artificial Intelligence, pages 1942\u20131948, 2011\n\n\nA. Sani, A. Lazaric, and R. Munos. Risk-Aversion in Multi-armed Bandits. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pages 3284\u20133292, 2012\n\n\nNicolas Galichet, M Sebag, and Olivier Teytaud. Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits. In ACML, 2013\n\n\nGartheeban Ganeshapillai, John Guttag, and Andrew W. Lo. Learning connections in financial time series. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 109\u2013117, 2013",
            "title": "Risk"
        },
        {
            "location": "/tutorials/risk/#aaai-2017-tutorial-on-risk-averse-decision-making-and-control",
            "text": "Presenters :   Marek Petrik, University of New Hampshire  Mohammad Ghavamzadeh, Adobe Research   Location : Continental 1-3, Ballroom Level",
            "title": "AAAI-2017: Tutorial on Risk-averse Decision Making and Control"
        },
        {
            "location": "/tutorials/risk/#schedule",
            "text": "9:00AM - 9:20AM : Introduction to risk-averse modeling  9:20AM - 9:40AM : Value at Risk and Conditional Value at Risk  9:40AM - 9:50AM :  Break  9:50AM - 10:30AM : Coherent Measures of Risk: Properties and methods  10:30AM - 11:00AM :  Coffee break  11:00AM - 12:00PM : Risk-averse reinforcement learning  12:00PM - 12:15PM :  Break  12:15PM - 12:45PM : Time consistent measures of risk",
            "title": "Schedule"
        },
        {
            "location": "/tutorials/risk/#slides",
            "text": "Coherent Measures of Risk (Dynamic and Static)  Risk-averse Reinforcement Learning",
            "title": "Slides"
        },
        {
            "location": "/tutorials/risk/#detailed-description",
            "text": "Traditional decision methods in artificial intelligence focus on maximizing the expected return (or minimizing the expected cost). This is appropriate when the decision-makers are risk-neutral. Yet, many decision-makers are  risk-sensitive  and are willing to give up some of the expected reward in order to protect against large losses. The desire to avoid risk when making decision was recognized early on, but developing appropriate models to capture risk has been challenging. Useful models of risk-aversion must be easy to understand and interpret for decision-makers; but they also must be general, flexible, and more importantly, they must produce tractable optimization problems.  The classical approach to modeling risk aversion is to use expected utilities, but they are difficult to specify and significantly complicate optimization methods. This tutorial focuses on the new approach to risk-aversion which is based convex measures of risk. Convex measures of risk replace the expectation operator by a more general operator which puts more weight on negative outcomes. Perhaps the most well-known risk measure is CVaR, which at level alpha computes the expectation of the lowest alpha-quantile of returns.   There has been tremendous progress in developing the theory and practice of risk measures since their introduction in the late 1990s. Researchers and practitioners have proposed and used many other risk measures besides CVaR and many stochastic optimization methods now work with convex risk measures. Due to the ease of modeling and optimization with them, convex risk measures have become the standard method for capturing risk sensitivity in operations research. Robust optimization, a related concept for modeling risk-aversion and avoidance, has flourished similarly.   In recent years, there has been growing interest in developing risk averse decision-making methods in artificial intelligence and machine learning. Risk-aversion is required to make machine learning relevant in many practical settings since solutions from risk neutral methods are often too risky in mission-critical problems. Convex risk measures and robust optimization are now being used in methods that range from classification, through multi-armed bandits, to reinforcement learning. While the general concept of risk measures is relatively simple, their true power can only be realized through deeper understanding. For example, integrating risk aversion with sequential decision-making requires overcoming a full set of challenges concerning time consistency. Our tutorial will shed light on these issues and provide numerous pointers for further research.",
            "title": "Detailed Description"
        },
        {
            "location": "/tutorials/risk/#goals-and-target-audience",
            "text": "This tutorial will introduce the tools and methodology of convex risk measures and robust optimization, developed in operations research and stochastic finance, to the machine learning community. The goal is to make these often complex results accessible and provide a starting point for people interested in exploring this research direction in greater detail. We will introduce basic concepts of risk measures and robust optimization, describe connections and advantages w.r.t. the existing methods, and describe how risk aversion can be used in sequential decision problems.   This tutorial should be of interest to researchers in any area that involves decision-making or control. This in particular includes the reinforcement learning and online learning communities, in which the application of risk aversion presents the most pitfalls. Risk aversion can also be important in classification and regression problems as several recent publications attest to. We plan to introduce the general risk-modeling framework and assume just knowledge of measure theoretical concepts, linear algebra, and basic optimization.",
            "title": "Goals and Target Audience"
        },
        {
            "location": "/tutorials/risk/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/tutorials/risk/#coherent-risk-measures",
            "text": "Philippe Artzner, Freddy Delbaen, Jean-marc Eber, and David Heath. Coherent Measures of Risk. Mathematical Finance, 9(June 1996):203\u2013228, 1999  R. Tyrrell Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk, 2:21\u201341, 2000  R. Tyrrell Rockafellar and Stanislav Uryasev. Conditional Value-At-Risk for General Loss Distributions. Journal of Banking and Finance, 26(7):1443\u20131471,   Alexander Schied. Risk measures and robust optimization problems. In Symposium on Probability and Stochastic Processes, 2004  Aharon Ben-Tal and Marc Teboulle. An Old-New Concept of Convex Risk Measures: The Optimized Certainty Equivalent. Mathematical Finance, 17:449\u2013476, 2007  Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Programming. SIAM, 2009  Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization. Princeton University Press, 2009  Hans Follmer and Alexander Schied. Stochastic Finance: An Introduction in Discrete Time. Walter de Gruyter, 3rd edition, 2011  A. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013",
            "title": "Coherent risk measures"
        },
        {
            "location": "/tutorials/risk/#sequential-decision-making",
            "text": "V. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems & Control Letters, 44:339\u2013346, 2001  V. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27:294\u2013311, 2002  Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints. Journal of Artificial Intelligence Research, 24:81\u2013108, 2005  E. Delage and S. Mannor. Percentile Optimization for Markov Decision Processes with Parameter Uncertainty. Operations Research, 58(1):203\u2013213, aug 2009  Andrzej Ruszczynski. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming B, 125(2):235\u2013261, jul 2010  Janusz Marecki and Pradeep Varakantham. Risk-Sensitive Planning in Partially Observable Environments. In Conference on Autonomous Agents and Multiagent Systems, 2010  Alexander Shapiro. Analysis of Stochastic Dual Dynamic Programming Method. European Journal of Operational Research, 209(1):63\u201372, 2011  Marek Petrik and Dharmashankar Subramanian. An approximate solution method for large risk-averse Markov decision processes. In Uncertainty in Artificial Intelligence(UAI), 2012  Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust Markov decision processes. Mathematics of Operations Research, 38(1):153\u2013183, apr 2013  A. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013  Vincent Guigues. SDDP for some interstage dependent risk-averse problems and application to hydro-thermal planning. Computational Optimization and Applications, pages 1\u201326, jul 2013  Aviv Tamar, Dotan Di Castro, and Shie Mannor. Temporal Difference Methods for the Variance of the Reward To Go. Proceedings of the 30th International Conference on Machine Learning, 28:495\u2013503, 2013  L.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In Advances in Neural Information Processing Systems, pages 252\u2013260, 2013  Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR Optimization in MDPs. In Neural Information Processing Systems (NIPS), pages 3509\u20133517, 2014  Dan A Iancu, Marek Petrik, and Dharmashankar Subramanian. Tight approximations of dynamic risk measures. Mathematics of Operations Research, 2015  Javier Garc and Fernando Fern. A Comprehensive Survey on Safe Reinforcement Learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015  Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the CVaR via Sampling. In AAAI Conference on Artificial Intelligence, pages 2993\u20132999, 2015  Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy Gradient for Coherent Risk Measures. In Neural Information Processing Systems (NIPS), pages 1468\u20131476, 2015  Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-Sensitive and Robust Decision-Making : a CVaR Optimization Approach. In Neural Information Processing Systems (NIPS), 2015  Aviv Tamar and Dotan Di Castro. Learning the Variance of the Reward-To-Go. Journal of Machine Learning Research, 17:1\u201336, 2016  L.A. Prashanth and Mohammad Ghavamzadeh. Variance-constrained Actor-Critic Algorithms for Discounted and Average Reward MDPs. Machine Learning Journal, 2016  Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, to appear, 2016",
            "title": "Sequential decision making"
        },
        {
            "location": "/tutorials/risk/#other-machine-learning",
            "text": "Stefano Ermon, Jon Conrad, Carla Gomes, and Bart Selman. Risk-Sensitive Policies for Sustainable Renewable Resource Allocation. Twenty-Second International Joint Conference on Artificial Intelligence, pages 1942\u20131948, 2011  A. Sani, A. Lazaric, and R. Munos. Risk-Aversion in Multi-armed Bandits. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pages 3284\u20133292, 2012  Nicolas Galichet, M Sebag, and Olivier Teytaud. Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits. In ACML, 2013  Gartheeban Ganeshapillai, John Guttag, and Andrew W. Lo. Learning connections in financial time series. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 109\u2013117, 2013",
            "title": "Other machine learning"
        }
    ]
}