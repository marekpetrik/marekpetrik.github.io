{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Marek Petrik I am an Assistant Professor in the Department of Computer Science at the University of New Hampshire . Before UNH, I was a Research Staff Member at the IBM's T. J. Watson Research Center in Yorktown, NY. I received my Ph.D. from University of Massachusetts Amherst in 2010. My advisor was: Shlomo Zilberstein . Research Interests I am interested in robust data-driven decision making . I am in particular interested in: Robust reinforcement learning with limited data sets Risk averse decision making Bayesian modeling methods for reinforcement learning My current applications are in the areas of: Reinforcement learning for mitigating the spread of invasive species Machine learning for integrated pest management I have previously worked on applications in precision agriculture, natural resources, renewable energy management, supply chains, advertising, and others. For more information about projects I have been involved in, please see my lab website . I am also a member of the artificial intelligence research group at UNH . Selected Recent Publications Fast Bellman Updates for Robust MDPs , Chin Pang Ho, Marek Petrik, Wolfram Wiesemann, International Conference on Machine Learning (ICML) , 2018. [ Full Paper ]. A Practical Method for Solving Contextual Bandit Problems Using Decision Trees , Adam N. Elmachtoub, Ryan McNellis, Marek Petrik, Sechan Oh, Uncertainty in Artificial Intelligence (UAI) , 2017. Value Directed Exploration in Multi-Armed Bandits with Structured Priors , Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml, Uncertainty in Artificial Intelligence (UAI) , 2017. Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, AAAI Conference , 2017. Safe Policy Improvement by Minimizing Robust Baseline Regret , Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, Conference on Neural Information Processing Systems (NIPS) , 2016. All Publications Contact Information If you would like to schedule a meeting with me, please see my calendar and change it to the weekly view to see my availability. Email: mpetrik@cs.unh.edu Office Phone: +1-603-862-2682 Department of Computer Science, Kingsbury W233 , Durham, NH 03824","title":"Home"},{"location":"#marek-petrik","text":"I am an Assistant Professor in the Department of Computer Science at the University of New Hampshire . Before UNH, I was a Research Staff Member at the IBM's T. J. Watson Research Center in Yorktown, NY. I received my Ph.D. from University of Massachusetts Amherst in 2010. My advisor was: Shlomo Zilberstein .","title":"Marek Petrik"},{"location":"#research-interests","text":"I am interested in robust data-driven decision making . I am in particular interested in: Robust reinforcement learning with limited data sets Risk averse decision making Bayesian modeling methods for reinforcement learning My current applications are in the areas of: Reinforcement learning for mitigating the spread of invasive species Machine learning for integrated pest management I have previously worked on applications in precision agriculture, natural resources, renewable energy management, supply chains, advertising, and others. For more information about projects I have been involved in, please see my lab website . I am also a member of the artificial intelligence research group at UNH .","title":"Research Interests"},{"location":"#selected-recent-publications","text":"Fast Bellman Updates for Robust MDPs , Chin Pang Ho, Marek Petrik, Wolfram Wiesemann, International Conference on Machine Learning (ICML) , 2018. [ Full Paper ]. A Practical Method for Solving Contextual Bandit Problems Using Decision Trees , Adam N. Elmachtoub, Ryan McNellis, Marek Petrik, Sechan Oh, Uncertainty in Artificial Intelligence (UAI) , 2017. Value Directed Exploration in Multi-Armed Bandits with Structured Priors , Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml, Uncertainty in Artificial Intelligence (UAI) , 2017. Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, AAAI Conference , 2017. Safe Policy Improvement by Minimizing Robust Baseline Regret , Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, Conference on Neural Information Processing Systems (NIPS) , 2016. All Publications","title":"Selected Recent Publications"},{"location":"#contact-information","text":"If you would like to schedule a meeting with me, please see my calendar and change it to the weekly view to see my availability. Email: mpetrik@cs.unh.edu Office Phone: +1-603-862-2682 Department of Computer Science, Kingsbury W233 , Durham, NH 03824","title":"Contact Information"},{"location":"CV/","text":"CV Please see the PDF version of my CV. About Marek Marek Petrik is an assistant professor of Computer Science at the University of New Hampshire. Until 2016, he was a research staff member at the Mathematical Sciences Department of IBM's T. J. Watson Research Center. He received his Ph.D. in Computer Science in 2010 from the University of Massachusetts, Amherst. His research focuses on developing better methods for safe, robust, and risk-averse reinforcement learning and their applications to domains that involve high-stakes decisions under uncertainty. Currently, Marek is involved with applications of robust reinforcement learning to controlling invasive species, pest monitoring and management in agriculture, and others.","title":"CV"},{"location":"CV/#cv","text":"Please see the PDF version of my CV.","title":"CV"},{"location":"CV/#about-marek","text":"Marek Petrik is an assistant professor of Computer Science at the University of New Hampshire. Until 2016, he was a research staff member at the Mathematical Sciences Department of IBM's T. J. Watson Research Center. He received his Ph.D. in Computer Science in 2010 from the University of Massachusetts, Amherst. His research focuses on developing better methods for safe, robust, and risk-averse reinforcement learning and their applications to domains that involve high-stakes decisions under uncertainty. Currently, Marek is involved with applications of robust reinforcement learning to controlling invasive species, pest monitoring and management in agriculture, and others.","title":"About Marek"},{"location":"code/","text":"Code Source code for some results in my publications. Robust Markov Decision Processes A C++ library for solving robust Markov decision processes. It also provides functionality for implementing simple MDP-based stochastic simulators. Can be used as a basis for reinforcement learning algorithms. Robust Aggregation for MDPs A Python library for simulating and solving large MDPs via an approximation by robust MDPs. Implements several old and new benchmark domains.","title":"Code"},{"location":"code/#code","text":"Source code for some results in my publications.","title":"Code"},{"location":"code/#robust-markov-decision-processes","text":"A C++ library for solving robust Markov decision processes. It also provides functionality for implementing simple MDP-based stochastic simulators. Can be used as a basis for reinforcement learning algorithms.","title":"Robust Markov Decision Processes"},{"location":"code/#robust-aggregation-for-mdps","text":"A Python library for simulating and solving large MDPs via an approximation by robust MDPs. Implements several old and new benchmark domains.","title":"Robust Aggregation for MDPs"},{"location":"publications/","text":"Publications and Presentations 2018 Fast Bellman Updates for Robust MDPs , Chin Pang Ho, Marek Petrik, Wolfram Wiesemann, International Conference on Machine Learning (ICML) , 2018. [ Full Paper ], [ Slides ]. Computing Robust Strategies for Managing Invasive Plants , Andreas Lydakis, Jenica Allen, Marek Petrik, Tim Szewczyk, AI for Wildlife Conservation Workshop at IJCAI/ICML, 2018. Low-rank Feature Selection for Reinforcement Learning , Bahram Behzadian, Marek Petrik, International Symposium on Artificial Intelligence and Mathematics , 2018. 2017 A Practical Method for Solving Contextual Bandit Problems Using Decision Trees , Adam N. Elmachtoub, Ryan McNellis, Marek Petrik, Sechan Oh, Uncertainty in Artificial Intelligence (UAI) , 2017. Value Directed Exploration in Multi-Armed Bandits with Structured Priors , Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml, Uncertainty in Artificial Intelligence (UAI) , 2017. Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, AAAI Conference , 2017. 2016 Safe Policy Improvement by Minimizing Robust Baseline Regret , Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, Conference on Neural Information Processing Systems (NIPS) , 2016. Interpretable Policies for Dynamic Product Recommendations , Marek Petrik, Ronny Luss, Uncertainty in Artificial Intelligence (UAI) , 2016. Building an Interpretable Recommender via Loss-Preserving Transformation , Amit Dhurandhar, Sechan Oh, Marek Petrik, 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016) . Safe Policy Improvement by Minimizing Robust Baseline Regret Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, 2016 ICML Workshop on Reliable Machine Learning in the Wild . 2015 Robust Policy Optimization with Baseline Guarantees , Yinlam Chow, Marek Petrik, Mohammad Ghavamzadeh, arXiv :1506.04514. Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy, arXiv :1510.04905. Tight Approximations of Dynamic Risk Measures , Dan Iancu, Marek Petrik, Dharmashankar Subramanian, Mathematics of Operations Research , 40(3), 2015. Finite-Sample Analysis of Proximal Gradient TD Algorithms , Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, Marek Petrik, Uncertainty in Artificial Intelligence (UAI) , 2015, ( Best Student Paper Award ). [ Appendix ] Optimal Threshold Control for Energy Arbitrage with Degradable Battery Storage , Marek Petrik, Xiaojian Wu, Uncertainty in Artificial Intelligence (UAI) , 2015. [ Appendix ] 2014 RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning , Marek Petrik, Dharmashankar Subramanian, Conference on Neural Information Processing Systems (NIPS), (spotlight) , 2014. [ Full Paper ]. Efficient and Accurate Methods for Updating Generalized Linear Models with Multiple Feature Additions , Amit Dhurandhar, Marek Petrik, Journal of Machine Learning Research 15:2607-2627 , 2014. [ bib ] Combining Social Media and Customer Behavior Analytics for Personalized Customer Engagements , Markus Ettl, Prateek Jain, Ronny Luss, Marek Petrik, Rajesh Ravi, Chitra Venkatramani, IBM Journal of Research Development , 58 (5/6) 7:1-7:12, 2014. 2013 Optimizing Deliveries in Agile Supply Chains with Demand Shocks , Francisco Barahona, Markus Ettl, Marek Petrik, Peter Rimshnick, Winter Simulation Conference , 2013. Solution Methods for Constrained Markov Decision Process with Continuous Probability Modulation , Janusz Marecki, Marek Petrik, Dharmashankar Subramanian, Conference on Uncertainty in Artificial Intelligence (UAI) , 2013. 2012 An Approximate Solution Method for Large Risk-Averse Markov Decision Processes , Marek Petrik and Dharmashankar Subramanian. Conference on Uncertainty in Artificial Intelligence (UAI) , 2012. Distributionally Robust Approach to Approximate Dynamic Programming , Marek Petrik, International Conference on Machine Learning (ICML) , 2012. Also presented at European Workshop on Reinforcement Learning , 2012. Extended Technical Report (includes proofs) . Optimizing the end-to-end value chain through demand shaping and advanced customer analytics , Brenda Dietrich, Markus Ettl, Roger D. Lederman, Marek Petrik, 11th International Symposium on Process Systems Engineering , 2012. 2011 The Price of Dynamic Inconsistency for Distortion Risk Measures , Pu Huang, Dan Iancu, Marek Petrik, Dharmashankar Subramanian. Technical Report , 2011. Linear Dynamic Programs for Resource Management , Marek Petrik and Shlomo Zilberstein, Conference on Artificial Intelligence (AAAI) [Computational Sustainability Track] , 2011. Robust Approximate Bilinear Programming for Value Function Approximation , Marek Petrik and Shlomo Zilberstein, Journal of Machine Learning Research 12(Oct):3027-3063 , 2011. 2010 Optimization-based Approximate Dynamic Programming , Marek Petrik, Ph.D. Dissertation , 2010. Also, the original double-spaced version , and the defense presentation . Feature Selection Using Regularization in Approximate Linear Program for Markov Decision Processes , Marek Petrik, Gavin Taylor, Ron Parr, Shlomo Zilberstein. International Conference on Machine Learning (ICML) 27 , 2010. Technical Report (includes proofs and algorithms): arXiv 1005.1860 . 2009 Robust Value Function Approximation Using Bilinear Programming , Marek Petrik and Shlomo Zilberstein, Conference on Neural Information Processing Systems (NIPS) 22 (spotlight) , 2009. Technical Report (includes proofs) UM-CS-2009-052 . A Bilinear Programming Approach for Multiagent Planning , Marek Petrik and Shlomo Zilberstein, Journal of Artificial Intelligence Research 35:235-274 , 2009. Hybrid Least-Squares Algorithms for Approximate Policy Evaluation , Jeff Johns, Marek Petrik, Sridhar Mahadevan, European Conference on Machine Learning , and Machine Learning journal, 2009. Constraint Relaxation in Approximate Linear Programs , Marek Petrik and Shlomo Zilberstein, International Conference on Machine Learning (ICML) , 2009. Robust Approximate Optimization for Large Scale Planning Problems , Marek Petrik, AAAI Doctoral Consortium , 2009. Blood Management Using Approximate Linear Programming , Marek Petrik and Shlomo Zilberstein, Presented at INFORMS Computing Society Meeting, Charleston, SC , 2009. 2008 A Successive Approximation Algorithm for Coordination Problems , Marek Petrik and Shlomo Zilberstein, 9th International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale, Florida , 2008. Biasing Approximate Dynamic Programming with a Lower Discount Factor ,Marek Petrik and Bruno Scherrer, Conference on Neural Information Processing Systems (NIPS) , 2008. Learning Heuristic Functions Through Approximate Linear Programming , Marek Petrik and Shlomo Zilberstein, International Conference on Automated Planning and Scheduling (ICAPS) , 2008. Interaction Structure and Dimensionality Reduction in Decentralized MDPs Martin Allen, Marek Petrik, Shlomo Zilberstein, The National Conference on Artificial Intelligence (AAAI) , 2008. Extented technical report #UM-CS-2008-11 . 2007 Anytime Coordination Using Separable Bilinear Programs , Marek Petrik, Shlomo Zilberstein, National Conference on Artificial Intelligence (AAAI) , 2007. An Analysis of Laplacian Methods for Value Function Approximation in MDPs , Marek Petrik, International Joint Conference on Artificial Intelligence (IJCAI) , 2007. Average-Reward Decentralized Markov Decision Processes , Marek Petrik, Shlomo Zilberstein, International Joint Conference on Artificial Intelligence (IJCAI) , 2007. 2006 Learning Parallel Portfolios of Algorithms , Marek Petrik, Shlomo Zilberstein, Annals of Mathematics and Artificial Intelligence, 48(1-2):85-106 , 2006 Learning Static Parallel Portfolios of Algorithms , Marek Petrik, Shlomo Zilberstein, International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale , 2006. Learning Parallel Portfolios of Algorithms , Marek Petrik, Diploma Thesis at Univerzita Komenskeho, June 7th 2005. The code , and presentation are also available. Statistically Optimal Combination of Algorithms , Marek Petrik, SOFSEM , 2005. (Best Student Poster).","title":"Publications"},{"location":"publications/#publications-and-presentations","text":"","title":"Publications and Presentations"},{"location":"publications/#2018","text":"Fast Bellman Updates for Robust MDPs , Chin Pang Ho, Marek Petrik, Wolfram Wiesemann, International Conference on Machine Learning (ICML) , 2018. [ Full Paper ], [ Slides ]. Computing Robust Strategies for Managing Invasive Plants , Andreas Lydakis, Jenica Allen, Marek Petrik, Tim Szewczyk, AI for Wildlife Conservation Workshop at IJCAI/ICML, 2018. Low-rank Feature Selection for Reinforcement Learning , Bahram Behzadian, Marek Petrik, International Symposium on Artificial Intelligence and Mathematics , 2018.","title":"2018"},{"location":"publications/#2017","text":"A Practical Method for Solving Contextual Bandit Problems Using Decision Trees , Adam N. Elmachtoub, Ryan McNellis, Marek Petrik, Sechan Oh, Uncertainty in Artificial Intelligence (UAI) , 2017. Value Directed Exploration in Multi-Armed Bandits with Structured Priors , Bence Cserna, Marek Petrik, Reazul Hasan Russel, Wheeler Ruml, Uncertainty in Artificial Intelligence (UAI) , 2017. Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, AAAI Conference , 2017.","title":"2017"},{"location":"publications/#2016","text":"Safe Policy Improvement by Minimizing Robust Baseline Regret , Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, Conference on Neural Information Processing Systems (NIPS) , 2016. Interpretable Policies for Dynamic Product Recommendations , Marek Petrik, Ronny Luss, Uncertainty in Artificial Intelligence (UAI) , 2016. Building an Interpretable Recommender via Loss-Preserving Transformation , Amit Dhurandhar, Sechan Oh, Marek Petrik, 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016) . Safe Policy Improvement by Minimizing Robust Baseline Regret Marek Petrik, Yinlam Chow, Mohammad Ghavamzadeh, 2016 ICML Workshop on Reliable Machine Learning in the Wild .","title":"2016"},{"location":"publications/#2015","text":"Robust Policy Optimization with Baseline Guarantees , Yinlam Chow, Marek Petrik, Mohammad Ghavamzadeh, arXiv :1506.04514. Robust Partially-Compressed Least-Squares , Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy, arXiv :1510.04905. Tight Approximations of Dynamic Risk Measures , Dan Iancu, Marek Petrik, Dharmashankar Subramanian, Mathematics of Operations Research , 40(3), 2015. Finite-Sample Analysis of Proximal Gradient TD Algorithms , Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, Marek Petrik, Uncertainty in Artificial Intelligence (UAI) , 2015, ( Best Student Paper Award ). [ Appendix ] Optimal Threshold Control for Energy Arbitrage with Degradable Battery Storage , Marek Petrik, Xiaojian Wu, Uncertainty in Artificial Intelligence (UAI) , 2015. [ Appendix ]","title":"2015"},{"location":"publications/#2014","text":"RAAM: The Benefits of Robustness in Approximating Aggregated MDPs in Reinforcement Learning , Marek Petrik, Dharmashankar Subramanian, Conference on Neural Information Processing Systems (NIPS), (spotlight) , 2014. [ Full Paper ]. Efficient and Accurate Methods for Updating Generalized Linear Models with Multiple Feature Additions , Amit Dhurandhar, Marek Petrik, Journal of Machine Learning Research 15:2607-2627 , 2014. [ bib ] Combining Social Media and Customer Behavior Analytics for Personalized Customer Engagements , Markus Ettl, Prateek Jain, Ronny Luss, Marek Petrik, Rajesh Ravi, Chitra Venkatramani, IBM Journal of Research Development , 58 (5/6) 7:1-7:12, 2014.","title":"2014"},{"location":"publications/#2013","text":"Optimizing Deliveries in Agile Supply Chains with Demand Shocks , Francisco Barahona, Markus Ettl, Marek Petrik, Peter Rimshnick, Winter Simulation Conference , 2013. Solution Methods for Constrained Markov Decision Process with Continuous Probability Modulation , Janusz Marecki, Marek Petrik, Dharmashankar Subramanian, Conference on Uncertainty in Artificial Intelligence (UAI) , 2013.","title":"2013"},{"location":"publications/#2012","text":"An Approximate Solution Method for Large Risk-Averse Markov Decision Processes , Marek Petrik and Dharmashankar Subramanian. Conference on Uncertainty in Artificial Intelligence (UAI) , 2012. Distributionally Robust Approach to Approximate Dynamic Programming , Marek Petrik, International Conference on Machine Learning (ICML) , 2012. Also presented at European Workshop on Reinforcement Learning , 2012. Extended Technical Report (includes proofs) . Optimizing the end-to-end value chain through demand shaping and advanced customer analytics , Brenda Dietrich, Markus Ettl, Roger D. Lederman, Marek Petrik, 11th International Symposium on Process Systems Engineering , 2012.","title":"2012"},{"location":"publications/#2011","text":"The Price of Dynamic Inconsistency for Distortion Risk Measures , Pu Huang, Dan Iancu, Marek Petrik, Dharmashankar Subramanian. Technical Report , 2011. Linear Dynamic Programs for Resource Management , Marek Petrik and Shlomo Zilberstein, Conference on Artificial Intelligence (AAAI) [Computational Sustainability Track] , 2011. Robust Approximate Bilinear Programming for Value Function Approximation , Marek Petrik and Shlomo Zilberstein, Journal of Machine Learning Research 12(Oct):3027-3063 , 2011.","title":"2011"},{"location":"publications/#2010","text":"Optimization-based Approximate Dynamic Programming , Marek Petrik, Ph.D. Dissertation , 2010. Also, the original double-spaced version , and the defense presentation . Feature Selection Using Regularization in Approximate Linear Program for Markov Decision Processes , Marek Petrik, Gavin Taylor, Ron Parr, Shlomo Zilberstein. International Conference on Machine Learning (ICML) 27 , 2010. Technical Report (includes proofs and algorithms): arXiv 1005.1860 .","title":"2010"},{"location":"publications/#2009","text":"Robust Value Function Approximation Using Bilinear Programming , Marek Petrik and Shlomo Zilberstein, Conference on Neural Information Processing Systems (NIPS) 22 (spotlight) , 2009. Technical Report (includes proofs) UM-CS-2009-052 . A Bilinear Programming Approach for Multiagent Planning , Marek Petrik and Shlomo Zilberstein, Journal of Artificial Intelligence Research 35:235-274 , 2009. Hybrid Least-Squares Algorithms for Approximate Policy Evaluation , Jeff Johns, Marek Petrik, Sridhar Mahadevan, European Conference on Machine Learning , and Machine Learning journal, 2009. Constraint Relaxation in Approximate Linear Programs , Marek Petrik and Shlomo Zilberstein, International Conference on Machine Learning (ICML) , 2009. Robust Approximate Optimization for Large Scale Planning Problems , Marek Petrik, AAAI Doctoral Consortium , 2009. Blood Management Using Approximate Linear Programming , Marek Petrik and Shlomo Zilberstein, Presented at INFORMS Computing Society Meeting, Charleston, SC , 2009.","title":"2009"},{"location":"publications/#2008","text":"A Successive Approximation Algorithm for Coordination Problems , Marek Petrik and Shlomo Zilberstein, 9th International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale, Florida , 2008. Biasing Approximate Dynamic Programming with a Lower Discount Factor ,Marek Petrik and Bruno Scherrer, Conference on Neural Information Processing Systems (NIPS) , 2008. Learning Heuristic Functions Through Approximate Linear Programming , Marek Petrik and Shlomo Zilberstein, International Conference on Automated Planning and Scheduling (ICAPS) , 2008. Interaction Structure and Dimensionality Reduction in Decentralized MDPs Martin Allen, Marek Petrik, Shlomo Zilberstein, The National Conference on Artificial Intelligence (AAAI) , 2008. Extented technical report #UM-CS-2008-11 .","title":"2008"},{"location":"publications/#2007","text":"Anytime Coordination Using Separable Bilinear Programs , Marek Petrik, Shlomo Zilberstein, National Conference on Artificial Intelligence (AAAI) , 2007. An Analysis of Laplacian Methods for Value Function Approximation in MDPs , Marek Petrik, International Joint Conference on Artificial Intelligence (IJCAI) , 2007. Average-Reward Decentralized Markov Decision Processes , Marek Petrik, Shlomo Zilberstein, International Joint Conference on Artificial Intelligence (IJCAI) , 2007.","title":"2007"},{"location":"publications/#2006","text":"Learning Parallel Portfolios of Algorithms , Marek Petrik, Shlomo Zilberstein, Annals of Mathematics and Artificial Intelligence, 48(1-2):85-106 , 2006 Learning Static Parallel Portfolios of Algorithms , Marek Petrik, Shlomo Zilberstein, International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale , 2006. Learning Parallel Portfolios of Algorithms , Marek Petrik, Diploma Thesis at Univerzita Komenskeho, June 7th 2005. The code , and presentation are also available. Statistically Optimal Combination of Algorithms , Marek Petrik, SOFSEM , 2005. (Best Student Poster).","title":"2006"},{"location":"teaching/","text":"Teaching Fall 2018 Reinforcement Learning Spring 2018 Introduction to Machine Learning Fall 2017 Reinforcement Learning Spring 2017 Introduction to Machine Learning Fall 2016 Advanced Topics in Machine Learning","title":"List All"},{"location":"teaching/#teaching","text":"","title":"Teaching"},{"location":"teaching/#fall-2018","text":"Reinforcement Learning","title":"Fall 2018"},{"location":"teaching/#spring-2018","text":"Introduction to Machine Learning","title":"Spring 2018"},{"location":"teaching/#fall-2017","text":"Reinforcement Learning","title":"Fall 2017"},{"location":"teaching/#spring-2017","text":"Introduction to Machine Learning","title":"Spring 2017"},{"location":"teaching/#fall-2016","text":"Advanced Topics in Machine Learning","title":"Fall 2016"},{"location":"teaching/adv_topics_ml_16/","text":"CS980: Advanced Machine Learning Where Kingsbury N233 When MW 3:40pm - 5:00pm What In this seminar, we will cover how to make good decisions using machine learning. Reinforcement learning and multi-armed bandits are just some of the methods that combine decision making with machine learning. These methods play a crucial role in countless real problems such as when personalizing websites, navigating robots, managing supply chains and revenue, and even when playing Go! We will cover how to trade off exploitation and exploration ; that is when to act with the current data versus acquiring additional data to make a better decision later. The exploration/exploitation tradeoff is a key challenge in reinforcement learning. We will also cover the related issues of reliability and robustness in machine learning. Some of the specific topics we will cover are: A/B testing and randomized experiments Optimizer's curse Thompson sampling, UCB, EXP3 (Robust) Markov decision processes Gittins and Whittle indices Tree and policy search Covariate shift The class will start with an overview of fundamental problems, methods, and techniques and will continue with student presentations and a research project. Pre-requisites Statistics and some linear algebra. Machine learning is not a pre-requisite. We will cover topics that are closely related to ML but do not depend on it. More details Please see the class website: https://bitbucket.org/2016advml/class/wiki","title":"CS980: Advanced Machine Learning #"},{"location":"teaching/adv_topics_ml_16/#cs980-advanced-machine-learning","text":"","title":"CS980: Advanced Machine Learning"},{"location":"teaching/adv_topics_ml_16/#where","text":"Kingsbury N233","title":"Where"},{"location":"teaching/adv_topics_ml_16/#when","text":"MW 3:40pm - 5:00pm","title":"When"},{"location":"teaching/adv_topics_ml_16/#what","text":"In this seminar, we will cover how to make good decisions using machine learning. Reinforcement learning and multi-armed bandits are just some of the methods that combine decision making with machine learning. These methods play a crucial role in countless real problems such as when personalizing websites, navigating robots, managing supply chains and revenue, and even when playing Go! We will cover how to trade off exploitation and exploration ; that is when to act with the current data versus acquiring additional data to make a better decision later. The exploration/exploitation tradeoff is a key challenge in reinforcement learning. We will also cover the related issues of reliability and robustness in machine learning. Some of the specific topics we will cover are: A/B testing and randomized experiments Optimizer's curse Thompson sampling, UCB, EXP3 (Robust) Markov decision processes Gittins and Whittle indices Tree and policy search Covariate shift The class will start with an overview of fundamental problems, methods, and techniques and will continue with student presentations and a research project.","title":"What"},{"location":"teaching/adv_topics_ml_16/#pre-requisites","text":"Statistics and some linear algebra. Machine learning is not a pre-requisite. We will cover topics that are closely related to ML but do not depend on it.","title":"Pre-requisites"},{"location":"teaching/adv_topics_ml_16/#more-details","text":"Please see the class website: https://bitbucket.org/2016advml/class/wiki","title":"More details"},{"location":"teaching/adv_topics_ml_17/","text":"CS980: Advanced ML: Reinforcement Learning Please see the class homepage for schedule and other up-to-date information. Where Kingsbury N233 When MW 3:40pm - 5:00pm What In this seminar, we will cover reinforcement learning , or how to make good decisions driven by data. The goal in reinforcement learning is to learn how to act while interacting with a dynamic and complex environment. Reinforcement learning methods can be applied in various domains, such as when managing ecosystems, optimizing website, to robotics, and healthcare. Our focus will be on reinforcement learning that can learn from batch data sets without interacting with the environment. The algorithms have to learn how to interact with the environment based on a historical data. These batch methods are important when the cost of failure is high, such as in healthcare of agriculture, when using trial and error is impractical. Some of the topics that we will cover are: Markov decision processes: Policy iteration, Value iteration, Linear programming Policy evaluation (online and offline): TD, LSTD Batch policy improvement: Q-learning, LSPI Convex optimization: Linear programming The focus of the class will be on depth rather than breadth. The main goal of the class is delivering an interesting reinforcement learning group project. As much as possible we will work on the same code-base using a subset of C++, Python, and R. The class will require independent study of reading materials and in-class group problem solving and paper discussions. Textbooks Follow the links for free online versions. Our main textbook will be: Szepesv\u00e1ri, C. (2010). Algorithms for Reinforcement Learning . Synthesis Lectures on Artificial Intelligence and Machine Learning, 4(1), 1\u2013103. Additional material can be found in: Puterman, M. L. (2005). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc. Sutton, R. S., & Barto, A. (1998). Reinforcement learning . MIT Press. Pre-requisites Statistics, some linear algebra and ideally familiarity with machine learning. More details","title":"2017: Advanced ML"},{"location":"teaching/adv_topics_ml_17/#cs980-advanced-ml-reinforcement-learning","text":"Please see the class homepage for schedule and other up-to-date information.","title":"CS980: Advanced ML: Reinforcement Learning"},{"location":"teaching/adv_topics_ml_17/#where","text":"Kingsbury N233","title":"Where"},{"location":"teaching/adv_topics_ml_17/#when","text":"MW 3:40pm - 5:00pm","title":"When"},{"location":"teaching/adv_topics_ml_17/#what","text":"In this seminar, we will cover reinforcement learning , or how to make good decisions driven by data. The goal in reinforcement learning is to learn how to act while interacting with a dynamic and complex environment. Reinforcement learning methods can be applied in various domains, such as when managing ecosystems, optimizing website, to robotics, and healthcare. Our focus will be on reinforcement learning that can learn from batch data sets without interacting with the environment. The algorithms have to learn how to interact with the environment based on a historical data. These batch methods are important when the cost of failure is high, such as in healthcare of agriculture, when using trial and error is impractical. Some of the topics that we will cover are: Markov decision processes: Policy iteration, Value iteration, Linear programming Policy evaluation (online and offline): TD, LSTD Batch policy improvement: Q-learning, LSPI Convex optimization: Linear programming The focus of the class will be on depth rather than breadth. The main goal of the class is delivering an interesting reinforcement learning group project. As much as possible we will work on the same code-base using a subset of C++, Python, and R. The class will require independent study of reading materials and in-class group problem solving and paper discussions.","title":"What"},{"location":"teaching/adv_topics_ml_17/#textbooks","text":"Follow the links for free online versions. Our main textbook will be: Szepesv\u00e1ri, C. (2010). Algorithms for Reinforcement Learning . Synthesis Lectures on Artificial Intelligence and Machine Learning, 4(1), 1\u2013103. Additional material can be found in: Puterman, M. L. (2005). Markov decision processes: Discrete stochastic dynamic programming. John Wiley & Sons, Inc. Sutton, R. S., & Barto, A. (1998). Reinforcement learning . MIT Press.","title":"Textbooks"},{"location":"teaching/adv_topics_ml_17/#pre-requisites","text":"Statistics, some linear algebra and ideally familiarity with machine learning.","title":"Pre-requisites"},{"location":"teaching/adv_topics_ml_17/#more-details","text":"","title":"More details"},{"location":"teaching/aml_18/","text":"CS980: Advanced ML: Reinforcement Learning Please see the class homepage for schedule and other up-to-date information. Where and When Kingsbury N233 MW 3:40pm - 5:00pm What This seminar will cover reinforcement learning . The goal in reinforcement learning is to learn how to act while interacting with a dynamic and complex environment. Instead of trying to cover all reinforcement learning topics, we will focus on the foundations needed to understand other concepts. Markov decision processes: Policy iteration, Value iteration, Linear programming Value Function Approximation: LSTD, LSPI, ALP, ABP Uncertainty: Bandits and Robust Markov Decision Processes We will use Python, R, or C++ and cover relevant topics from linear algebra, mathematical optimization, and statistics as needed. More Info Please see the class homepage .","title":"CS980: Advanced ML: Reinforcement Learning #"},{"location":"teaching/aml_18/#cs980-advanced-ml-reinforcement-learning","text":"Please see the class homepage for schedule and other up-to-date information.","title":"CS980: Advanced ML: Reinforcement Learning"},{"location":"teaching/aml_18/#where-and-when","text":"Kingsbury N233 MW 3:40pm - 5:00pm","title":"Where and When"},{"location":"teaching/aml_18/#what","text":"This seminar will cover reinforcement learning . The goal in reinforcement learning is to learn how to act while interacting with a dynamic and complex environment. Instead of trying to cover all reinforcement learning topics, we will focus on the foundations needed to understand other concepts. Markov decision processes: Policy iteration, Value iteration, Linear programming Value Function Approximation: LSTD, LSPI, ALP, ABP Uncertainty: Bandits and Robust Markov Decision Processes We will use Python, R, or C++ and cover relevant topics from linear algebra, mathematical optimization, and statistics as needed.","title":"What"},{"location":"teaching/aml_18/#more-info","text":"Please see the class homepage .","title":"More Info"},{"location":"teaching/intro_ml_17/","text":"CS780 / CS880: Introduction to Machine Learning When and Where Tue & Thu, 12:40 pm - 2:00 pm Kingsbury N133 See class overview for more information on textbooks, syllabus, assignments, office hours, and grading. Assignments Please use Piazza for questions about assignments. Assignment Due Date Assignment 1 2/14/17 at 12:40PM Assignment 2 2/21/17 at 12:40PM Assignment 3 3/09/17 at 12:40PM Assignment 4 4/06/17 at 12:40PM Assignment 5 4/20/17 at 12:40PM Syllabus Date Slides Reading Notebooks 1/26 Statistical learning ISL 1,2 (html) (RMD) 1/31 Linear regression I ISL 3.1-2 (html) (RMD) 2/02 No class 2/07 Linear regression II ISL 3.3-6 2/09 No class 2/14 Logistic regression ISL 4.1-3 (html) (RMD) 2/16 LDA, QDA, Bayes ISL 4.4-6 2/21 Cross-validation ISL 5 2/23 Model selection ISL 6.1-6.2 2/28 Dimensionality ISL 6.3-6.4 3/2 PCA ML/MAP ISL 10.1-2 ML PCA 3/6 Clustering and EM ISL 10.3-5 kmeans 3/9 Midterm Review ISL 1-6, 10 3/21 Midterm 3/23 Linear algebra LAO 1.1-2,2,3 3/28 LA in ML LAR linear algebra 3/30 LA in ML LAR linear algebra 4/04 SVM ISL 9 4/06 Decision trees and boosting ISL 8 4/11 Nonlinear methods ISL 7 4/13 Recommender systems 4/18 Bayes nets MLP 10 4/20 Reinforcement learning RL 4/25 Final exam review 4/27 Project presentations (Graduate) 5/02 Deep learning and big data DL 5/04 Project presentations (Undergraduate) Project See the project overview for details on the details of deliverables. The deliverable are due by the end of the day (midnight). Date Deliverable Page Limit 2/24 Project description and data sources 1 3/07 Evaluation methodology 1 3/23 Method and literature overview 2 4/06 Preliminary results 3 4/27 Final report 7 Exams See practice questions for questions you should be able to answer to be ready for the midterm and final exams. Date Exam 3/21 Midterm (take home) Textbooks Main reference: ISL: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning More in-depth material: ESL: Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer Series in Statistics (2nd ed.) Related topics: LAO: Hefferon, J. Linear Algebra (2017) LA: Strang, G. Introduction to Linear Algebra . (2016) Also see: online lectures LAR: Introductory Linear Agebra with R CO: Boyd, S., & Vandenberghe, L. (2004). Convex Optimization . RL: Sutton, R. S., & Barto, A. (2012). Reinforcement learning . 2nd edition (forthcoming?) RLA: Szepesvari, C. (2013), Algorithms for Reinforcement Learning DL: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning MLP: Murphy, K (2012). Machine Learning, A Probabilistic Perspective. See class overview for more information on the textbook. Class Content The goal of this class is to teach you how to use machine learning to understand data and make predictions in practice. The class will cover the fundamental concepts and algorithms in machine learning and data science as well as a wide variety of practical algorithms. The main topics we will cover are: The maximum likelihood principle Regression : Linear regression Classification : Logistic regression and linear discriminant analysis Cross-validation, bootstrap, and over-fitting Model selection : Regularization, Lasso Nonlinear models : Decision trees, Support vector machines Unsupervised : Principal component analysis, k-means Advanced topics : Bayes nets and deep learning The graduate version of the class will cover the same topics in greater depth. Programming Language The class will involve hand-on data analysis using machine learning methods. The recommended language for programming assignments is R which is an excellent tool for statistical analysis and machine learning. No prior knowledge of R is needed or expected ; the book and lecture will cover a gentle introduction to the language. Experienced students may also choose other alternatives, such as Python or Matlab. Pre-requisites Basic programming skills (scripting languages like Python are OK) and some familiarity with statistics and calculus. If in doubt, please email me.","title":"2017: Intro to ML"},{"location":"teaching/intro_ml_17/#cs780-cs880-introduction-to-machine-learning","text":"","title":"CS780 / CS880: Introduction to Machine Learning"},{"location":"teaching/intro_ml_17/#when-and-where","text":"Tue & Thu, 12:40 pm - 2:00 pm Kingsbury N133 See class overview for more information on textbooks, syllabus, assignments, office hours, and grading.","title":"When and Where"},{"location":"teaching/intro_ml_17/#assignments","text":"Please use Piazza for questions about assignments. Assignment Due Date Assignment 1 2/14/17 at 12:40PM Assignment 2 2/21/17 at 12:40PM Assignment 3 3/09/17 at 12:40PM Assignment 4 4/06/17 at 12:40PM Assignment 5 4/20/17 at 12:40PM","title":"Assignments"},{"location":"teaching/intro_ml_17/#syllabus","text":"Date Slides Reading Notebooks 1/26 Statistical learning ISL 1,2 (html) (RMD) 1/31 Linear regression I ISL 3.1-2 (html) (RMD) 2/02 No class 2/07 Linear regression II ISL 3.3-6 2/09 No class 2/14 Logistic regression ISL 4.1-3 (html) (RMD) 2/16 LDA, QDA, Bayes ISL 4.4-6 2/21 Cross-validation ISL 5 2/23 Model selection ISL 6.1-6.2 2/28 Dimensionality ISL 6.3-6.4 3/2 PCA ML/MAP ISL 10.1-2 ML PCA 3/6 Clustering and EM ISL 10.3-5 kmeans 3/9 Midterm Review ISL 1-6, 10 3/21 Midterm 3/23 Linear algebra LAO 1.1-2,2,3 3/28 LA in ML LAR linear algebra 3/30 LA in ML LAR linear algebra 4/04 SVM ISL 9 4/06 Decision trees and boosting ISL 8 4/11 Nonlinear methods ISL 7 4/13 Recommender systems 4/18 Bayes nets MLP 10 4/20 Reinforcement learning RL 4/25 Final exam review 4/27 Project presentations (Graduate) 5/02 Deep learning and big data DL 5/04 Project presentations (Undergraduate)","title":"Syllabus"},{"location":"teaching/intro_ml_17/#project","text":"See the project overview for details on the details of deliverables. The deliverable are due by the end of the day (midnight). Date Deliverable Page Limit 2/24 Project description and data sources 1 3/07 Evaluation methodology 1 3/23 Method and literature overview 2 4/06 Preliminary results 3 4/27 Final report 7","title":"Project"},{"location":"teaching/intro_ml_17/#exams","text":"See practice questions for questions you should be able to answer to be ready for the midterm and final exams. Date Exam 3/21 Midterm (take home)","title":"Exams"},{"location":"teaching/intro_ml_17/#textbooks","text":"","title":"Textbooks"},{"location":"teaching/intro_ml_17/#main-reference","text":"ISL: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning","title":"Main reference:"},{"location":"teaching/intro_ml_17/#more-in-depth-material","text":"ESL: Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer Series in Statistics (2nd ed.)","title":"More in-depth material:"},{"location":"teaching/intro_ml_17/#related-topics","text":"LAO: Hefferon, J. Linear Algebra (2017) LA: Strang, G. Introduction to Linear Algebra . (2016) Also see: online lectures LAR: Introductory Linear Agebra with R CO: Boyd, S., & Vandenberghe, L. (2004). Convex Optimization . RL: Sutton, R. S., & Barto, A. (2012). Reinforcement learning . 2nd edition (forthcoming?) RLA: Szepesvari, C. (2013), Algorithms for Reinforcement Learning DL: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning MLP: Murphy, K (2012). Machine Learning, A Probabilistic Perspective. See class overview for more information on the textbook.","title":"Related topics:"},{"location":"teaching/intro_ml_17/#class-content","text":"The goal of this class is to teach you how to use machine learning to understand data and make predictions in practice. The class will cover the fundamental concepts and algorithms in machine learning and data science as well as a wide variety of practical algorithms. The main topics we will cover are: The maximum likelihood principle Regression : Linear regression Classification : Logistic regression and linear discriminant analysis Cross-validation, bootstrap, and over-fitting Model selection : Regularization, Lasso Nonlinear models : Decision trees, Support vector machines Unsupervised : Principal component analysis, k-means Advanced topics : Bayes nets and deep learning The graduate version of the class will cover the same topics in greater depth.","title":"Class Content"},{"location":"teaching/intro_ml_17/#programming-language","text":"The class will involve hand-on data analysis using machine learning methods. The recommended language for programming assignments is R which is an excellent tool for statistical analysis and machine learning. No prior knowledge of R is needed or expected ; the book and lecture will cover a gentle introduction to the language. Experienced students may also choose other alternatives, such as Python or Matlab.","title":"Programming Language"},{"location":"teaching/intro_ml_17/#pre-requisites","text":"Basic programming skills (scripting languages like Python are OK) and some familiarity with statistics and calculus. If in doubt, please email me.","title":"Pre-requisites"},{"location":"teaching/ml_18/","text":"CS780 / CS880: Machine Learning Please see the main class website for detailed up to date information. When and Where Mon & Wed, 11:10 am - 12:40 pm in Kingsbury N113 Class Content The goal of this class is to teach you how to use machine learning to understand data and make predictions in practice. The class will cover the fundamental concepts and algorithms in machine learning and data science as well as a wide variety of practical algorithms. The main topics we will cover are: The maximum likelihood principle Regression : Linear regression Classification : Logistic regression and linear discriminant analysis Cross-validation, bootstrap, and over-fitting Model selection : Regularization, Lasso Nonlinear models : Decision trees, Support vector machines Unsupervised : Principal component analysis, k-means Advanced topics : Bayes nets and deep learning The graduate version of the class will cover the same topics in greater depth. Programming Language The class will involve hand-on data analysis using machine learning methods. The recommended language for programming assignments is R which is an excellent tool for statistical analysis and machine learning. No prior knowledge of R is needed or expected ; the book and lecture will cover a gentle introduction to the language. Experienced students may also choose other alternatives, such as Python or Matlab. We recommend using the free R Studio for completing programming assignments. R Notebooks are very convenient for producing reproducible reports and we encourage you to use them. Jupyter is a similar alternative for Python. Pre-requisites Basic programming skills (scripting languages like Python are OK) and some familiarity with statistics and calculus. If in doubt, please email me. See class overview for more information on textbooks, syllabus, assignments, office hours, and grading. Textbooks Main Reference: ISL : James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning","title":"2018: Machine Learning"},{"location":"teaching/ml_18/#cs780-cs880-machine-learning","text":"Please see the main class website for detailed up to date information.","title":"CS780 / CS880: Machine Learning"},{"location":"teaching/ml_18/#when-and-where","text":"Mon & Wed, 11:10 am - 12:40 pm in Kingsbury N113","title":"When and Where"},{"location":"teaching/ml_18/#class-content","text":"The goal of this class is to teach you how to use machine learning to understand data and make predictions in practice. The class will cover the fundamental concepts and algorithms in machine learning and data science as well as a wide variety of practical algorithms. The main topics we will cover are: The maximum likelihood principle Regression : Linear regression Classification : Logistic regression and linear discriminant analysis Cross-validation, bootstrap, and over-fitting Model selection : Regularization, Lasso Nonlinear models : Decision trees, Support vector machines Unsupervised : Principal component analysis, k-means Advanced topics : Bayes nets and deep learning The graduate version of the class will cover the same topics in greater depth.","title":"Class Content"},{"location":"teaching/ml_18/#programming-language","text":"The class will involve hand-on data analysis using machine learning methods. The recommended language for programming assignments is R which is an excellent tool for statistical analysis and machine learning. No prior knowledge of R is needed or expected ; the book and lecture will cover a gentle introduction to the language. Experienced students may also choose other alternatives, such as Python or Matlab. We recommend using the free R Studio for completing programming assignments. R Notebooks are very convenient for producing reproducible reports and we encourage you to use them. Jupyter is a similar alternative for Python.","title":"Programming Language"},{"location":"teaching/ml_18/#pre-requisites","text":"Basic programming skills (scripting languages like Python are OK) and some familiarity with statistics and calculus. If in doubt, please email me. See class overview for more information on textbooks, syllabus, assignments, office hours, and grading.","title":"Pre-requisites"},{"location":"teaching/ml_18/#textbooks","text":"","title":"Textbooks"},{"location":"teaching/ml_18/#main-reference","text":"ISL : James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning","title":"Main Reference:"},{"location":"tutorials/risk/","text":"AAAI-2017: Tutorial on Risk-averse Decision Making and Control Presenters : Marek Petrik, University of New Hampshire Mohammad Ghavamzadeh, Adobe Research Location : Continental 1-3, Ballroom Level Schedule 9:00AM - 9:20AM : Introduction to risk-averse modeling 9:20AM - 9:40AM : Value at Risk and Conditional Value at Risk 9:40AM - 9:50AM : Break 9:50AM - 10:30AM : Coherent Measures of Risk: Properties and methods 10:30AM - 11:00AM : Coffee break 11:00AM - 12:00PM : Risk-averse reinforcement learning 12:00PM - 12:15PM : Break 12:15PM - 12:45PM : Time consistent measures of risk Slides Coherent Measures of Risk (Dynamic and Static) Risk-averse Reinforcement Learning Detailed Description Traditional decision methods in artificial intelligence focus on maximizing the expected return (or minimizing the expected cost). This is appropriate when the decision-makers are risk-neutral. Yet, many decision-makers are risk-sensitive and are willing to give up some of the expected reward in order to protect against large losses. The desire to avoid risk when making decision was recognized early on, but developing appropriate models to capture risk has been challenging. Useful models of risk-aversion must be easy to understand and interpret for decision-makers; but they also must be general, flexible, and more importantly, they must produce tractable optimization problems. The classical approach to modeling risk aversion is to use expected utilities, but they are difficult to specify and significantly complicate optimization methods. This tutorial focuses on the new approach to risk-aversion which is based convex measures of risk. Convex measures of risk replace the expectation operator by a more general operator which puts more weight on negative outcomes. Perhaps the most well-known risk measure is CVaR, which at level alpha computes the expectation of the lowest alpha-quantile of returns. There has been tremendous progress in developing the theory and practice of risk measures since their introduction in the late 1990s. Researchers and practitioners have proposed and used many other risk measures besides CVaR and many stochastic optimization methods now work with convex risk measures. Due to the ease of modeling and optimization with them, convex risk measures have become the standard method for capturing risk sensitivity in operations research. Robust optimization, a related concept for modeling risk-aversion and avoidance, has flourished similarly. In recent years, there has been growing interest in developing risk averse decision-making methods in artificial intelligence and machine learning. Risk-aversion is required to make machine learning relevant in many practical settings since solutions from risk neutral methods are often too risky in mission-critical problems. Convex risk measures and robust optimization are now being used in methods that range from classification, through multi-armed bandits, to reinforcement learning. While the general concept of risk measures is relatively simple, their true power can only be realized through deeper understanding. For example, integrating risk aversion with sequential decision-making requires overcoming a full set of challenges concerning time consistency. Our tutorial will shed light on these issues and provide numerous pointers for further research. Goals and Target Audience This tutorial will introduce the tools and methodology of convex risk measures and robust optimization, developed in operations research and stochastic finance, to the machine learning community. The goal is to make these often complex results accessible and provide a starting point for people interested in exploring this research direction in greater detail. We will introduce basic concepts of risk measures and robust optimization, describe connections and advantages w.r.t. the existing methods, and describe how risk aversion can be used in sequential decision problems. This tutorial should be of interest to researchers in any area that involves decision-making or control. This in particular includes the reinforcement learning and online learning communities, in which the application of risk aversion presents the most pitfalls. Risk aversion can also be important in classification and regression problems as several recent publications attest to. We plan to introduce the general risk-modeling framework and assume just knowledge of measure theoretical concepts, linear algebra, and basic optimization. References Coherent risk measures Philippe Artzner, Freddy Delbaen, Jean-marc Eber, and David Heath. Coherent Measures of Risk. Mathematical Finance, 9(June 1996):203\u2013228, 1999 R. Tyrrell Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk, 2:21\u201341, 2000 R. Tyrrell Rockafellar and Stanislav Uryasev. Conditional Value-At-Risk for General Loss Distributions. Journal of Banking and Finance, 26(7):1443\u20131471, Alexander Schied. Risk measures and robust optimization problems. In Symposium on Probability and Stochastic Processes, 2004 Aharon Ben-Tal and Marc Teboulle. An Old-New Concept of Convex Risk Measures: The Optimized Certainty Equivalent. Mathematical Finance, 17:449\u2013476, 2007 Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Programming. SIAM, 2009 Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization. Princeton University Press, 2009 Hans Follmer and Alexander Schied. Stochastic Finance: An Introduction in Discrete Time. Walter de Gruyter, 3rd edition, 2011 A. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013 Sequential decision making V. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems & Control Letters, 44:339\u2013346, 2001 V. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27:294\u2013311, 2002 Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints. Journal of Artificial Intelligence Research, 24:81\u2013108, 2005 E. Delage and S. Mannor. Percentile Optimization for Markov Decision Processes with Parameter Uncertainty. Operations Research, 58(1):203\u2013213, aug 2009 Andrzej Ruszczynski. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming B, 125(2):235\u2013261, jul 2010 Janusz Marecki and Pradeep Varakantham. Risk-Sensitive Planning in Partially Observable Environments. In Conference on Autonomous Agents and Multiagent Systems, 2010 Alexander Shapiro. Analysis of Stochastic Dual Dynamic Programming Method. European Journal of Operational Research, 209(1):63\u201372, 2011 Marek Petrik and Dharmashankar Subramanian. An approximate solution method for large risk-averse Markov decision processes. In Uncertainty in Artificial Intelligence(UAI), 2012 Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust Markov decision processes. Mathematics of Operations Research, 38(1):153\u2013183, apr 2013 A. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013 Vincent Guigues. SDDP for some interstage dependent risk-averse problems and application to hydro-thermal planning. Computational Optimization and Applications, pages 1\u201326, jul 2013 Aviv Tamar, Dotan Di Castro, and Shie Mannor. Temporal Difference Methods for the Variance of the Reward To Go. Proceedings of the 30th International Conference on Machine Learning, 28:495\u2013503, 2013 L.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In Advances in Neural Information Processing Systems, pages 252\u2013260, 2013 Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR Optimization in MDPs. In Neural Information Processing Systems (NIPS), pages 3509\u20133517, 2014 Dan A Iancu, Marek Petrik, and Dharmashankar Subramanian. Tight approximations of dynamic risk measures. Mathematics of Operations Research, 2015 Javier Garc and Fernando Fern. A Comprehensive Survey on Safe Reinforcement Learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015 Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the CVaR via Sampling. In AAAI Conference on Artificial Intelligence, pages 2993\u20132999, 2015 Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy Gradient for Coherent Risk Measures. In Neural Information Processing Systems (NIPS), pages 1468\u20131476, 2015 Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-Sensitive and Robust Decision-Making : a CVaR Optimization Approach. In Neural Information Processing Systems (NIPS), 2015 Aviv Tamar and Dotan Di Castro. Learning the Variance of the Reward-To-Go. Journal of Machine Learning Research, 17:1\u201336, 2016 L.A. Prashanth and Mohammad Ghavamzadeh. Variance-constrained Actor-Critic Algorithms for Discounted and Average Reward MDPs. Machine Learning Journal, 2016 Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, to appear, 2016 Other machine learning Stefano Ermon, Jon Conrad, Carla Gomes, and Bart Selman. Risk-Sensitive Policies for Sustainable Renewable Resource Allocation. Twenty-Second International Joint Conference on Artificial Intelligence, pages 1942\u20131948, 2011 A. Sani, A. Lazaric, and R. Munos. Risk-Aversion in Multi-armed Bandits. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pages 3284\u20133292, 2012 Nicolas Galichet, M Sebag, and Olivier Teytaud. Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits. In ACML, 2013 Gartheeban Ganeshapillai, John Guttag, and Andrew W. Lo. Learning connections in financial time series. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 109\u2013117, 2013","title":"Risk"},{"location":"tutorials/risk/#aaai-2017-tutorial-on-risk-averse-decision-making-and-control","text":"Presenters : Marek Petrik, University of New Hampshire Mohammad Ghavamzadeh, Adobe Research Location : Continental 1-3, Ballroom Level","title":"AAAI-2017: Tutorial on Risk-averse Decision Making and Control"},{"location":"tutorials/risk/#schedule","text":"9:00AM - 9:20AM : Introduction to risk-averse modeling 9:20AM - 9:40AM : Value at Risk and Conditional Value at Risk 9:40AM - 9:50AM : Break 9:50AM - 10:30AM : Coherent Measures of Risk: Properties and methods 10:30AM - 11:00AM : Coffee break 11:00AM - 12:00PM : Risk-averse reinforcement learning 12:00PM - 12:15PM : Break 12:15PM - 12:45PM : Time consistent measures of risk","title":"Schedule"},{"location":"tutorials/risk/#slides","text":"Coherent Measures of Risk (Dynamic and Static) Risk-averse Reinforcement Learning","title":"Slides"},{"location":"tutorials/risk/#detailed-description","text":"Traditional decision methods in artificial intelligence focus on maximizing the expected return (or minimizing the expected cost). This is appropriate when the decision-makers are risk-neutral. Yet, many decision-makers are risk-sensitive and are willing to give up some of the expected reward in order to protect against large losses. The desire to avoid risk when making decision was recognized early on, but developing appropriate models to capture risk has been challenging. Useful models of risk-aversion must be easy to understand and interpret for decision-makers; but they also must be general, flexible, and more importantly, they must produce tractable optimization problems. The classical approach to modeling risk aversion is to use expected utilities, but they are difficult to specify and significantly complicate optimization methods. This tutorial focuses on the new approach to risk-aversion which is based convex measures of risk. Convex measures of risk replace the expectation operator by a more general operator which puts more weight on negative outcomes. Perhaps the most well-known risk measure is CVaR, which at level alpha computes the expectation of the lowest alpha-quantile of returns. There has been tremendous progress in developing the theory and practice of risk measures since their introduction in the late 1990s. Researchers and practitioners have proposed and used many other risk measures besides CVaR and many stochastic optimization methods now work with convex risk measures. Due to the ease of modeling and optimization with them, convex risk measures have become the standard method for capturing risk sensitivity in operations research. Robust optimization, a related concept for modeling risk-aversion and avoidance, has flourished similarly. In recent years, there has been growing interest in developing risk averse decision-making methods in artificial intelligence and machine learning. Risk-aversion is required to make machine learning relevant in many practical settings since solutions from risk neutral methods are often too risky in mission-critical problems. Convex risk measures and robust optimization are now being used in methods that range from classification, through multi-armed bandits, to reinforcement learning. While the general concept of risk measures is relatively simple, their true power can only be realized through deeper understanding. For example, integrating risk aversion with sequential decision-making requires overcoming a full set of challenges concerning time consistency. Our tutorial will shed light on these issues and provide numerous pointers for further research.","title":"Detailed Description"},{"location":"tutorials/risk/#goals-and-target-audience","text":"This tutorial will introduce the tools and methodology of convex risk measures and robust optimization, developed in operations research and stochastic finance, to the machine learning community. The goal is to make these often complex results accessible and provide a starting point for people interested in exploring this research direction in greater detail. We will introduce basic concepts of risk measures and robust optimization, describe connections and advantages w.r.t. the existing methods, and describe how risk aversion can be used in sequential decision problems. This tutorial should be of interest to researchers in any area that involves decision-making or control. This in particular includes the reinforcement learning and online learning communities, in which the application of risk aversion presents the most pitfalls. Risk aversion can also be important in classification and regression problems as several recent publications attest to. We plan to introduce the general risk-modeling framework and assume just knowledge of measure theoretical concepts, linear algebra, and basic optimization.","title":"Goals and Target Audience"},{"location":"tutorials/risk/#references","text":"","title":"References"},{"location":"tutorials/risk/#coherent-risk-measures","text":"Philippe Artzner, Freddy Delbaen, Jean-marc Eber, and David Heath. Coherent Measures of Risk. Mathematical Finance, 9(June 1996):203\u2013228, 1999 R. Tyrrell Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of Risk, 2:21\u201341, 2000 R. Tyrrell Rockafellar and Stanislav Uryasev. Conditional Value-At-Risk for General Loss Distributions. Journal of Banking and Finance, 26(7):1443\u20131471, Alexander Schied. Risk measures and robust optimization problems. In Symposium on Probability and Stochastic Processes, 2004 Aharon Ben-Tal and Marc Teboulle. An Old-New Concept of Convex Risk Measures: The Optimized Certainty Equivalent. Mathematical Finance, 17:449\u2013476, 2007 Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on Stochastic Programming. SIAM, 2009 Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization. Princeton University Press, 2009 Hans Follmer and Alexander Schied. Stochastic Finance: An Introduction in Discrete Time. Walter de Gruyter, 3rd edition, 2011 A. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013","title":"Coherent risk measures"},{"location":"tutorials/risk/#sequential-decision-making","text":"V. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems & Control Letters, 44:339\u2013346, 2001 V. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27:294\u2013311, 2002 Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under constraints. Journal of Artificial Intelligence Research, 24:81\u2013108, 2005 E. Delage and S. Mannor. Percentile Optimization for Markov Decision Processes with Parameter Uncertainty. Operations Research, 58(1):203\u2013213, aug 2009 Andrzej Ruszczynski. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming B, 125(2):235\u2013261, jul 2010 Janusz Marecki and Pradeep Varakantham. Risk-Sensitive Planning in Partially Observable Environments. In Conference on Autonomous Agents and Multiagent Systems, 2010 Alexander Shapiro. Analysis of Stochastic Dual Dynamic Programming Method. European Journal of Operational Research, 209(1):63\u201372, 2011 Marek Petrik and Dharmashankar Subramanian. An approximate solution method for large risk-averse Markov decision processes. In Uncertainty in Artificial Intelligence(UAI), 2012 Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust Markov decision processes. Mathematics of Operations Research, 38(1):153\u2013183, apr 2013 A. Shapiro, W. Tekaya, J. P. da Costa, and M. P. Soares. Risk-neutral and risk-averse stochastic dual dynamic programming method. European Journal of Operational Research, 224(2):375\u2013391, 2013 Vincent Guigues. SDDP for some interstage dependent risk-averse problems and application to hydro-thermal planning. Computational Optimization and Applications, pages 1\u201326, jul 2013 Aviv Tamar, Dotan Di Castro, and Shie Mannor. Temporal Difference Methods for the Variance of the Reward To Go. Proceedings of the 30th International Conference on Machine Learning, 28:495\u2013503, 2013 L.A. Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In Advances in Neural Information Processing Systems, pages 252\u2013260, 2013 Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR Optimization in MDPs. In Neural Information Processing Systems (NIPS), pages 3509\u20133517, 2014 Dan A Iancu, Marek Petrik, and Dharmashankar Subramanian. Tight approximations of dynamic risk measures. Mathematics of Operations Research, 2015 Javier Garc and Fernando Fern. A Comprehensive Survey on Safe Reinforcement Learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015 Aviv Tamar, Yonatan Glassner, and Shie Mannor. Optimizing the CVaR via Sampling. In AAAI Conference on Artificial Intelligence, pages 2993\u20132999, 2015 Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy Gradient for Coherent Risk Measures. In Neural Information Processing Systems (NIPS), pages 1468\u20131476, 2015 Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-Sensitive and Robust Decision-Making : a CVaR Optimization Approach. In Neural Information Processing Systems (NIPS), 2015 Aviv Tamar and Dotan Di Castro. Learning the Variance of the Reward-To-Go. Journal of Machine Learning Research, 17:1\u201336, 2016 L.A. Prashanth and Mohammad Ghavamzadeh. Variance-constrained Actor-Critic Algorithms for Discounted and Average Reward MDPs. Machine Learning Journal, 2016 Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, to appear, 2016","title":"Sequential decision making"},{"location":"tutorials/risk/#other-machine-learning","text":"Stefano Ermon, Jon Conrad, Carla Gomes, and Bart Selman. Risk-Sensitive Policies for Sustainable Renewable Resource Allocation. Twenty-Second International Joint Conference on Artificial Intelligence, pages 1942\u20131948, 2011 A. Sani, A. Lazaric, and R. Munos. Risk-Aversion in Multi-armed Bandits. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pages 3284\u20133292, 2012 Nicolas Galichet, M Sebag, and Olivier Teytaud. Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits. In ACML, 2013 Gartheeban Ganeshapillai, John Guttag, and Andrew W. Lo. Learning connections in financial time series. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 109\u2013117, 2013","title":"Other machine learning"}]}